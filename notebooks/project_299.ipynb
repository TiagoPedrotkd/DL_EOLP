{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c40d1e",
   "metadata": {},
   "source": [
    "#  Projeto Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab3b56",
   "metadata": {},
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60948300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, Input\n",
    "import subprocess\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fee9c",
   "metadata": {},
   "source": [
    "## Carregamento do CSV e Verificação de imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63baac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"metadata.csv\" \n",
    "meta_data = pd.read_csv(metadata_path)\n",
    "\n",
    "base_dir = \"/Users/joaosantos/Documents/Mestrado Joao/2 semestre/Deep Learning/rare_specie\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ea26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image_exists(image_file):\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if image_file in files:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8719bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_images = []\n",
    "for index, row in meta_data.iterrows():\n",
    "    image_path = row['file_path']  \n",
    "    image_name = os.path.basename(image_path)  \n",
    "    if not check_image_exists(image_name):\n",
    "        missing_images.append(image_path)\n",
    "\n",
    "print(f\"Número de imagens em falta: {len(missing_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388756dc",
   "metadata": {},
   "source": [
    "## Load Images\n",
    "Carregar imagens visto que cada pasta é uma classe (familia) e dar one hot enconding nas classes , ou seja, \n",
    "X_data são imagens e y_data são rótulos/classes codificados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19fd03",
   "metadata": {},
   "source": [
    "Resizing usando 224x224, pois medidas mais baixas fazem perder qualidade de imagem e consequentemente pioram a performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef49b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover fundo da imagem\n",
    "def remove_background(input_path, output_path):\n",
    "    try:\n",
    "        print(f\"Removendo fundo da imagem: {input_path}\")\n",
    "        # Executa o rembg para remover o fundo\n",
    "        result = subprocess.run(['rembg', 'i', input_path, output_path], check=True, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Fundo removido com sucesso de {input_path}. Salvo em {output_path}\")\n",
    "        else:\n",
    "            print(f\"Erro ao processar {input_path}: {result.stderr}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Erro ao processar {input_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99996dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcão para carregar imagens e dar one hot enconding\n",
    "def load_local_images(base_dir, image_size=(299, 299)):\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # Obter os rótulos \n",
    "    labels = os.listdir(base_dir)\n",
    "    labels = [label for label in labels if os.path.isdir(os.path.join(base_dir, label)) and not label.startswith('.')]\n",
    "    label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    for label in labels:\n",
    "        class_dir = os.path.join(base_dir, label)\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"Erro:{img_path}\")\n",
    "                continue\n",
    "            img = cv2.resize(img, image_size)\n",
    "            X_data.append(img)\n",
    "            y_data.append(label_map[label])\n",
    "\n",
    "    X_data = np.array(X_data, dtype=\"float32\")  \n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    # One-hot encoding\n",
    "    num_classes = len(labels)\n",
    "    y_data = to_categorical(y_data, num_classes=num_classes)\n",
    "\n",
    "    print(f\"Classes encontradas: {labels}\")\n",
    "    return X_data, y_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee10db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes encontradas: ['chordata_balaenidae', 'chordata_ardeidae', 'chordata_pleuronectidae', 'chordata_goodeidae', 'chordata_plethodontidae', 'chordata_labridae', 'cnidaria_agariciidae', 'mollusca_cardiidae', 'chordata_cervidae', 'arthropoda_papilionidae', 'chordata_dasypodidae', 'chordata_turdidae', 'chordata_recurvirostridae', 'chordata_tropiduridae', 'chordata_vombatidae', 'cnidaria_dendrophylliidae', 'chordata_carettochelyidae', 'chordata_hylobatidae', 'chordata_soricidae', 'cnidaria_faviidae', 'chordata_balaenicipitidae', 'chordata_strigopidae', 'chordata_gliridae', 'chordata_daubentoniidae', 'chordata_paradisaeidae', 'arthropoda_pseudophasmatidae', 'chordata_diomedeidae', 'chordata_bovidae', 'chordata_parulidae', 'chordata_laridae', 'chordata_vireonidae', 'chordata_cebidae', 'chordata_callitrichidae', 'arthropoda_formicidae', 'chordata_diplodactylidae', 'cnidaria_fungiidae', 'chordata_vespertilionidae', 'chordata_ambystomatidae', 'chordata_scolopacidae', 'chordata_equidae', 'chordata_phasianidae', 'chordata_alcedinidae', 'arthropoda_cerambycidae', 'cnidaria_meandrinidae', 'chordata_mimidae', 'chordata_lamnidae', 'chordata_otididae', 'chordata_serranidae', 'chordata_ciconiidae', 'chordata_vangidae', 'chordata_cryptobranchidae', 'chordata_salmonidae', 'chordata_rallidae', 'chordata_columbidae', 'chordata_gekkonidae', 'arthropoda_coenagrionidae', 'chordata_latimeriidae', 'chordata_pontoporiidae', 'chordata_pythonidae', 'chordata_polyprionidae', 'chordata_squalidae', 'chordata_caprimulgidae', 'chordata_trochilidae', 'chordata_hominidae', 'chordata_cercopithecidae', 'chordata_gymnuridae', 'chordata_charadriidae', 'chordata_balaenopteridae', 'chordata_hynobiidae', 'chordata_pristidae', 'chordata_cuculidae', 'cnidaria_siderastreidae', 'mollusca_conidae', 'chordata_syngnathidae', 'mollusca_zonitidae', 'chordata_merlucciidae', 'arthropoda_gomphidae', 'chordata_squatinidae', 'chordata_lacertidae', 'chordata_myliobatidae', 'chordata_leporidae', 'chordata_otariidae', 'chordata_ursidae', 'chordata_bombycillidae', 'chordata_phyllostomidae', 'chordata_scincidae', 'chordata_pittidae', 'cnidaria_acroporidae', 'chordata_muscicapidae', 'chordata_hemiscylliidae', 'chordata_aotidae', 'arthropoda_tettigoniidae', 'chordata_cheirogaleidae', 'chordata_scombridae', 'chordata_hexanchidae', 'chordata_pardalotidae', 'arthropoda_apidae', 'chordata_chaetodontidae', 'arthropoda_lucanidae', 'chordata_sciuridae', 'chordata_hyaenidae', 'arthropoda_nymphalidae', 'chordata_carcharhinidae', 'chordata_brachypteraciidae', 'cnidaria_merulinidae', 'chordata_somniosidae', 'chordata_strigidae', 'chordata_dasyatidae', 'chordata_colubridae', 'chordata_agamidae', 'echinodermata_stichopodidae', 'chordata_cetorhinidae', 'chordata_falconidae', 'cnidaria_lobophylliidae', 'chordata_ranidae', 'chordata_dasyuridae', 'chordata_viperidae', 'chordata_indriidae', 'chordata_motacillidae', 'chordata_chamaeleonidae', 'arthropoda_palinuridae', 'chordata_procellariidae', 'arthropoda_theraphosidae', 'chordata_giraffidae', 'chordata_spheniscidae', 'chordata_ctenomyidae', 'chordata_chelidae', 'mollusca_unionidae', 'chordata_delphinidae', 'chordata_bucerotidae', 'arthropoda_platystictidae', 'chordata_lemuridae', 'chordata_callorhinchidae', 'chordata_salamandridae', 'chordata_manidae', 'chordata_rajidae', 'chordata_anguidae', 'chordata_pangasiidae', 'chordata_odontophoridae', 'chordata_gavialidae', 'chordata_siluridae', 'chordata_psittaculidae', 'chordata_thraupidae', 'chordata_estrildidae', 'cnidaria_euphylliidae', 'cnidaria_diploastraeidae', 'chordata_mesitornithidae', 'chordata_cracidae', 'chordata_geoemydidae', 'cnidaria_helioporidae', 'chordata_elapidae', 'chordata_crocodylidae', 'chordata_cricetidae', 'arthropoda_attelabidae', 'chordata_mustelidae', 'chordata_glareolidae', 'chordata_balistidae', 'chordata_alopiidae', 'arthropoda_triopsidae', 'chordata_rhinodermatidae', 'chordata_cacatuidae', 'cnidaria_pocilloporidae', 'chordata_dendrobatidae', 'chordata_iguanidae', 'chordata_cyprinodontidae', 'chordata_dactyloidae', 'chordata_phyllomedusidae', 'chordata_sphyrnidae', 'chordata_trionychidae', 'chordata_testudinidae', 'chordata_arthroleptidae', 'mollusca_haliotidae', 'chordata_atelidae', 'chordata_emydidae', 'chordata_ramphastidae', 'chordata_rhyacotritonidae', 'chordata_dalatiidae', 'chordata_mantellidae', 'chordata_percidae', 'chordata_accipitridae', 'chordata_burhinidae', 'chordata_nesospingidae', 'chordata_chelydridae', 'chordata_potoroidae', 'chordata_psittacidae', 'arthropoda_carabidae', 'chordata_alligatoridae', 'chordata_podocnemididae', 'chordata_sparidae', 'arthropoda_pisauridae', 'chordata_lutjanidae', 'chordata_phrynosomatidae', 'chordata_urolophidae', 'chordata_anatidae', 'chordata_bufonidae', 'chordata_megapodiidae', 'chordata_rhacophoridae', 'chordata_acipenseridae', 'chordata_fringillidae', 'chordata_cheloniidae', 'chordata_albulidae', 'chordata_trogonidae']\n",
      "X_train shape: (7189, 299, 299, 3), y_train shape: (7189, 202)\n",
      "X_val shape: (2397, 299, 299, 3), y_val shape: (2397, 202)\n",
      "X_test shape: (2397, 299, 299, 3), y_test shape: (2397, 202)\n"
     ]
    }
   ],
   "source": [
    "#Carregar os dados\n",
    "image_size = (299, 299)\n",
    "X_data, y_data = load_local_images(base_dir, image_size=image_size)\n",
    "\n",
    "# Primeiro split: separar o conjunto de teste\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    ")\n",
    "\n",
    "# Segundo split: separar o conjunto de validação do conjunto de treino temporário\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "# Augmentation\n",
    "train_aug = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=40,          # rotação maior\n",
    "    zoom_range=0.3,              # zoom mais forte\n",
    "    width_shift_range=0.2,       # deslocamento horizontal maior\n",
    "    height_shift_range=0.2,      # deslocamento vertical maior\n",
    "    shear_range=20,              # distorções\n",
    "    brightness_range=[0.7, 1.3], # brilho variável\n",
    "    horizontal_flip=True         # flip horizontal\n",
    ")\n",
    "val_aug = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_aug = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Datasets\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = train_aug.flow(X_train, y_train, batch_size=batch_size)\n",
    "val_dataset = val_aug.flow(X_val, y_val, batch_size=batch_size)\n",
    "test_dataset = test_aug.flow(X_test, y_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e85a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter y_train de one-hot para label (índice)\n",
    "y_train_labels = np.argmax(y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a7754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(1.9771727172717273), 1: np.float64(0.9885863586358636), 2: np.float64(1.9771727172717273), 3: np.float64(1.9771727172717273), 4: np.float64(0.21968585747463634), 5: np.float64(0.9885863586358636), 6: np.float64(0.4942931793179318), 7: np.float64(0.9885863586358636), 8: np.float64(1.9771727172717273), 9: np.float64(1.9771727172717273), 10: np.float64(1.9771727172717273), 11: np.float64(0.9885863586358636), 12: np.float64(1.9771727172717273), 13: np.float64(1.9771727172717273), 14: np.float64(1.9771727172717273), 15: np.float64(1.9771727172717273), 16: np.float64(1.9771727172717273), 17: np.float64(0.659057572423909), 18: np.float64(1.9771727172717273), 19: np.float64(0.9885863586358636), 20: np.float64(1.9771727172717273), 21: np.float64(1.9771727172717273), 22: np.float64(1.9771727172717273), 23: np.float64(1.9771727172717273), 24: np.float64(1.9771727172717273), 25: np.float64(1.9771727172717273), 26: np.float64(0.3295287862119545), 27: np.float64(0.2471465896589659), 28: np.float64(1.9771727172717273), 29: np.float64(0.3954345434543454), 30: np.float64(1.9771727172717273), 31: np.float64(0.659057572423909), 32: np.float64(0.4942931793179318), 33: np.float64(0.20336633663366335), 34: np.float64(0.9885863586358636), 35: np.float64(1.9771727172717273), 36: np.float64(0.9885863586358636), 37: np.float64(0.9885863586358636), 38: np.float64(0.9885863586358636), 39: np.float64(1.9771727172717273), 40: np.float64(1.9771727172717273), 41: np.float64(1.9771727172717273), 42: np.float64(1.9771727172717273), 43: np.float64(1.9771727172717273), 44: np.float64(1.9771727172717273), 45: np.float64(1.9771727172717273), 46: np.float64(1.9771727172717273), 47: np.float64(0.659057572423909), 48: np.float64(0.659057572423909), 49: np.float64(1.9771727172717273), 50: np.float64(0.9885863586358636), 51: np.float64(1.9771727172717273), 52: np.float64(1.9771727172717273), 53: np.float64(1.9771727172717273), 54: np.float64(1.9771727172717273), 55: np.float64(1.9771727172717273), 56: np.float64(1.9771727172717273), 57: np.float64(1.9771727172717273), 58: np.float64(0.9885863586358636), 59: np.float64(1.9771727172717273), 60: np.float64(1.9771727172717273), 61: np.float64(1.9771727172717273), 62: np.float64(0.659057572423909), 63: np.float64(1.9771727172717273), 64: np.float64(0.1977172717271727), 65: np.float64(1.9771727172717273), 66: np.float64(0.9885863586358636), 67: np.float64(1.9771727172717273), 68: np.float64(1.9771727172717273), 69: np.float64(1.9771727172717273), 70: np.float64(1.9771727172717273), 71: np.float64(1.9771727172717273), 72: np.float64(1.9771727172717273), 73: np.float64(0.9885863586358636), 74: np.float64(1.9771727172717273), 75: np.float64(1.9771727172717273), 76: np.float64(0.9885863586358636), 77: np.float64(1.9771727172717273), 78: np.float64(0.9885863586358636), 79: np.float64(0.659057572423909), 80: np.float64(0.9885863586358636), 81: np.float64(0.9885863586358636), 82: np.float64(0.9885863586358636), 83: np.float64(1.9771727172717273), 84: np.float64(1.9771727172717273), 85: np.float64(1.9771727172717273), 86: np.float64(1.9771727172717273), 87: np.float64(0.2824532453245325), 88: np.float64(1.9771727172717273), 89: np.float64(1.9771727172717273), 90: np.float64(0.9885863586358636), 91: np.float64(1.9771727172717273), 92: np.float64(1.9771727172717273), 93: np.float64(1.9771727172717273), 94: np.float64(0.9885863586358636), 95: np.float64(1.9771727172717273), 96: np.float64(0.3954345434543454), 97: np.float64(0.9885863586358636), 98: np.float64(1.9771727172717273), 99: np.float64(1.9771727172717273), 100: np.float64(1.9771727172717273), 101: np.float64(1.9771727172717273), 102: np.float64(0.21968585747463634), 103: np.float64(1.9771727172717273), 104: np.float64(0.9885863586358636), 105: np.float64(1.9771727172717273), 106: np.float64(1.9771727172717273), 107: np.float64(0.3954345434543454), 108: np.float64(0.659057572423909), 109: np.float64(0.9885863586358636), 110: np.float64(0.9885863586358636), 111: np.float64(1.9771727172717273), 112: np.float64(0.9885863586358636), 113: np.float64(1.9771727172717273), 114: np.float64(0.659057572423909), 115: np.float64(1.9771727172717273), 116: np.float64(1.9771727172717273), 117: np.float64(1.9771727172717273), 118: np.float64(1.9771727172717273), 119: np.float64(1.0467384973791496), 120: np.float64(1.9771727172717273), 121: np.float64(0.659057572423909), 122: np.float64(1.9771727172717273), 123: np.float64(1.9771727172717273), 124: np.float64(0.4942931793179318), 125: np.float64(1.9771727172717273), 126: np.float64(1.0467384973791496), 127: np.float64(0.9885863586358636), 128: np.float64(0.4942931793179318), 129: np.float64(0.2824532453245325), 130: np.float64(1.9771727172717273), 131: np.float64(0.9885863586358636), 132: np.float64(1.9771727172717273), 133: np.float64(0.21968585747463634), 134: np.float64(0.9885863586358636), 135: np.float64(0.659057572423909), 136: np.float64(0.9885863586358636), 137: np.float64(1.9771727172717273), 138: np.float64(1.9771727172717273), 139: np.float64(1.9771727172717273), 140: np.float64(2.093476994758299), 141: np.float64(0.9885863586358636), 142: np.float64(0.659057572423909), 143: np.float64(1.9771727172717273), 144: np.float64(0.659057572423909), 145: np.float64(1.9771727172717273), 146: np.float64(1.9771727172717273), 147: np.float64(0.9885863586358636), 148: np.float64(0.659057572423909), 149: np.float64(1.9771727172717273), 150: np.float64(1.9771727172717273), 151: np.float64(0.9885863586358636), 152: np.float64(1.9771727172717273), 153: np.float64(1.9771727172717273), 154: np.float64(0.659057572423909), 155: np.float64(1.9771727172717273), 156: np.float64(0.9885863586358636), 157: np.float64(0.9885863586358636), 158: np.float64(1.9771727172717273), 159: np.float64(1.9771727172717273), 160: np.float64(1.9771727172717273), 161: np.float64(0.9885863586358636), 162: np.float64(1.9771727172717273), 163: np.float64(0.4942931793179318), 164: np.float64(1.9771727172717273), 165: np.float64(0.1977172717271727), 166: np.float64(1.9771727172717273), 167: np.float64(0.659057572423909), 168: np.float64(1.9771727172717273), 169: np.float64(0.9885863586358636), 170: np.float64(1.9771727172717273), 171: np.float64(1.9771727172717273), 172: np.float64(0.3954345434543454), 173: np.float64(0.9885863586358636), 174: np.float64(1.9771727172717273), 175: np.float64(0.9885863586358636), 176: np.float64(1.9771727172717273), 177: np.float64(1.9771727172717273), 178: np.float64(1.9771727172717273), 179: np.float64(0.5012550550829731), 180: np.float64(0.9885863586358636), 181: np.float64(1.9771727172717273), 182: np.float64(1.9771727172717273), 183: np.float64(1.9771727172717273), 184: np.float64(0.4942931793179318), 185: np.float64(0.9885863586358636), 186: np.float64(1.9771727172717273), 187: np.float64(1.9771727172717273), 188: np.float64(0.9885863586358636), 189: np.float64(1.9771727172717273), 190: np.float64(1.9771727172717273), 191: np.float64(1.9771727172717273), 192: np.float64(1.9771727172717273), 193: np.float64(0.3295287862119545), 194: np.float64(0.3954345434543454), 195: np.float64(1.9771727172717273), 196: np.float64(0.9885863586358636), 197: np.float64(0.659057572423909), 198: np.float64(0.9885863586358636), 199: np.float64(1.9771727172717273), 200: np.float64(1.9771727172717273), 201: np.float64(1.9771727172717273)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calcular os pesos\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_labels),\n",
    "    y=y_train_labels\n",
    ")\n",
    "\n",
    "# Transformar para dict (como o Keras espera)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7fd33",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a566aa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaosantos/.pyenv/versions/3.10.12/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaosantos/.pyenv/versions/3.10.12/lib/python3.10/site-packages/keras/src/models/functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(None, 299, 299, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 5.01572, saving model to melhor_modelo.keras\n",
      "225/225 - 303s - 1s/step - AUC: 0.5696 - accuracy: 0.0134 - loss: 5.3409 - val_AUC: 0.7214 - val_accuracy: 0.1026 - val_loss: 5.0157 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 5.01572 to 4.72052, saving model to melhor_modelo.keras\n",
      "225/225 - 294s - 1s/step - AUC: 0.6981 - accuracy: 0.0558 - loss: 4.9949 - val_AUC: 0.8345 - val_accuracy: 0.2520 - val_loss: 4.7205 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 4.72052 to 4.33322, saving model to melhor_modelo.keras\n",
      "225/225 - 293s - 1s/step - AUC: 0.7894 - accuracy: 0.1242 - loss: 4.6460 - val_AUC: 0.9094 - val_accuracy: 0.3567 - val_loss: 4.3332 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 4.33322 to 3.88546, saving model to melhor_modelo.keras\n",
      "225/225 - 1149s - 5s/step - AUC: 0.8534 - accuracy: 0.1991 - loss: 4.2098 - val_AUC: 0.9427 - val_accuracy: 0.4043 - val_loss: 3.8855 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 3.88546 to 3.45062, saving model to melhor_modelo.keras\n",
      "225/225 - 641s - 3s/step - AUC: 0.8898 - accuracy: 0.2622 - loss: 3.7334 - val_AUC: 0.9568 - val_accuracy: 0.4401 - val_loss: 3.4506 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 3.45062 to 3.09789, saving model to melhor_modelo.keras\n",
      "225/225 - 286s - 1s/step - AUC: 0.9093 - accuracy: 0.2960 - loss: 3.3545 - val_AUC: 0.9623 - val_accuracy: 0.4681 - val_loss: 3.0979 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 3.09789 to 2.83688, saving model to melhor_modelo.keras\n",
      "225/225 - 287s - 1s/step - AUC: 0.9224 - accuracy: 0.3355 - loss: 3.0747 - val_AUC: 0.9666 - val_accuracy: 0.4919 - val_loss: 2.8369 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 2.83688 to 2.63307, saving model to melhor_modelo.keras\n",
      "225/225 - 290s - 1s/step - AUC: 0.9295 - accuracy: 0.3564 - loss: 2.8290 - val_AUC: 0.9688 - val_accuracy: 0.5086 - val_loss: 2.6331 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 2.63307 to 2.46478, saving model to melhor_modelo.keras\n",
      "225/225 - 298s - 1s/step - AUC: 0.9300 - accuracy: 0.3706 - loss: 2.6822 - val_AUC: 0.9703 - val_accuracy: 0.5236 - val_loss: 2.4648 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 2.46478 to 2.33620, saving model to melhor_modelo.keras\n",
      "225/225 - 296s - 1s/step - AUC: 0.9398 - accuracy: 0.3960 - loss: 2.4967 - val_AUC: 0.9714 - val_accuracy: 0.5232 - val_loss: 2.3362 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 2.33620 to 2.23533, saving model to melhor_modelo.keras\n",
      "225/225 - 296s - 1s/step - AUC: 0.9435 - accuracy: 0.4117 - loss: 2.3831 - val_AUC: 0.9715 - val_accuracy: 0.5378 - val_loss: 2.2353 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 2.23533 to 2.13111, saving model to melhor_modelo.keras\n",
      "225/225 - 295s - 1s/step - AUC: 0.9455 - accuracy: 0.4238 - loss: 2.2779 - val_AUC: 0.9734 - val_accuracy: 0.5457 - val_loss: 2.1311 - learning_rate: 1.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 2.13111 to 2.04970, saving model to melhor_modelo.keras\n",
      "225/225 - 297s - 1s/step - AUC: 0.9492 - accuracy: 0.4333 - loss: 2.1820 - val_AUC: 0.9741 - val_accuracy: 0.5561 - val_loss: 2.0497 - learning_rate: 1.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 2.04970 to 1.97673, saving model to melhor_modelo.keras\n",
      "225/225 - 301s - 1s/step - AUC: 0.9505 - accuracy: 0.4472 - loss: 2.1052 - val_AUC: 0.9743 - val_accuracy: 0.5653 - val_loss: 1.9767 - learning_rate: 1.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 1.97673 to 1.92526, saving model to melhor_modelo.keras\n",
      "225/225 - 299s - 1s/step - AUC: 0.9549 - accuracy: 0.4647 - loss: 2.0280 - val_AUC: 0.9754 - val_accuracy: 0.5653 - val_loss: 1.9253 - learning_rate: 1.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 1.92526 to 1.87240, saving model to melhor_modelo.keras\n",
      "225/225 - 298s - 1s/step - AUC: 0.9529 - accuracy: 0.4642 - loss: 1.9900 - val_AUC: 0.9754 - val_accuracy: 0.5749 - val_loss: 1.8724 - learning_rate: 1.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 1.87240 to 1.82162, saving model to melhor_modelo.keras\n",
      "225/225 - 298s - 1s/step - AUC: 0.9585 - accuracy: 0.4686 - loss: 1.9245 - val_AUC: 0.9762 - val_accuracy: 0.5853 - val_loss: 1.8216 - learning_rate: 1.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 1.82162 to 1.78148, saving model to melhor_modelo.keras\n",
      "225/225 - 296s - 1s/step - AUC: 0.9573 - accuracy: 0.4806 - loss: 1.8734 - val_AUC: 0.9763 - val_accuracy: 0.5903 - val_loss: 1.7815 - learning_rate: 1.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 1.78148 to 1.74861, saving model to melhor_modelo.keras\n",
      "225/225 - 304s - 1s/step - AUC: 0.9613 - accuracy: 0.4999 - loss: 1.7964 - val_AUC: 0.9763 - val_accuracy: 0.5907 - val_loss: 1.7486 - learning_rate: 1.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 1.74861 to 1.72017, saving model to melhor_modelo.keras\n",
      "225/225 - 297s - 1s/step - AUC: 0.9600 - accuracy: 0.5009 - loss: 1.7765 - val_AUC: 0.9764 - val_accuracy: 0.5949 - val_loss: 1.7202 - learning_rate: 1.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 1.72017 to 1.68165, saving model to melhor_modelo.keras\n",
      "225/225 - 298s - 1s/step - AUC: 0.9617 - accuracy: 0.5049 - loss: 1.7433 - val_AUC: 0.9766 - val_accuracy: 0.5999 - val_loss: 1.6817 - learning_rate: 1.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 1.68165 to 1.65112, saving model to melhor_modelo.keras\n",
      "225/225 - 300s - 1s/step - AUC: 0.9631 - accuracy: 0.5120 - loss: 1.6846 - val_AUC: 0.9763 - val_accuracy: 0.6049 - val_loss: 1.6511 - learning_rate: 1.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 1.65112 to 1.62553, saving model to melhor_modelo.keras\n",
      "225/225 - 297s - 1s/step - AUC: 0.9644 - accuracy: 0.5186 - loss: 1.6548 - val_AUC: 0.9779 - val_accuracy: 0.6108 - val_loss: 1.6255 - learning_rate: 1.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 1.62553 to 1.60236, saving model to melhor_modelo.keras\n",
      "225/225 - 293s - 1s/step - AUC: 0.9637 - accuracy: 0.5250 - loss: 1.6308 - val_AUC: 0.9775 - val_accuracy: 0.6087 - val_loss: 1.6024 - learning_rate: 1.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 1.60236 to 1.58265, saving model to melhor_modelo.keras\n",
      "225/225 - 294s - 1s/step - AUC: 0.9652 - accuracy: 0.5278 - loss: 1.6116 - val_AUC: 0.9767 - val_accuracy: 0.6162 - val_loss: 1.5826 - learning_rate: 1.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 1.58265 to 1.55581, saving model to melhor_modelo.keras\n",
      "225/225 - 281s - 1s/step - AUC: 0.9666 - accuracy: 0.5348 - loss: 1.5503 - val_AUC: 0.9767 - val_accuracy: 0.6183 - val_loss: 1.5558 - learning_rate: 1.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 1.55581 to 1.53810, saving model to melhor_modelo.keras\n",
      "225/225 - 324s - 1s/step - AUC: 0.9665 - accuracy: 0.5405 - loss: 1.5425 - val_AUC: 0.9774 - val_accuracy: 0.6170 - val_loss: 1.5381 - learning_rate: 1.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 1.53810 to 1.52307, saving model to melhor_modelo.keras\n",
      "225/225 - 1496s - 7s/step - AUC: 0.9679 - accuracy: 0.5465 - loss: 1.4992 - val_AUC: 0.9769 - val_accuracy: 0.6249 - val_loss: 1.5231 - learning_rate: 1.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 1.52307 to 1.50859, saving model to melhor_modelo.keras\n",
      "225/225 - 585s - 3s/step - AUC: 0.9689 - accuracy: 0.5552 - loss: 1.4938 - val_AUC: 0.9774 - val_accuracy: 0.6249 - val_loss: 1.5086 - learning_rate: 1.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 1.50859 to 1.48011, saving model to melhor_modelo.keras\n",
      "225/225 - 686s - 3s/step - AUC: 0.9706 - accuracy: 0.5617 - loss: 1.4364 - val_AUC: 0.9770 - val_accuracy: 0.6295 - val_loss: 1.4801 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Epoch 1/10\n",
      "225/225 - 3299s - 15s/step - AUC: 0.9528 - accuracy: 0.4141 - loss: 2.4848 - val_AUC: 0.9790 - val_accuracy: 0.5786 - val_loss: 1.8659 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "225/225 - 9953s - 44s/step - AUC: 0.9628 - accuracy: 0.4654 - loss: 2.0843 - val_AUC: 0.9785 - val_accuracy: 0.5870 - val_loss: 1.7843 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "225/225 - 2849s - 13s/step - AUC: 0.9691 - accuracy: 0.4955 - loss: 1.8471 - val_AUC: 0.9795 - val_accuracy: 0.5995 - val_loss: 1.6967 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "225/225 - 5568s - 25s/step - AUC: 0.9714 - accuracy: 0.5229 - loss: 1.6868 - val_AUC: 0.9800 - val_accuracy: 0.6041 - val_loss: 1.6349 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "225/225 - 3103s - 14s/step - AUC: 0.9724 - accuracy: 0.5287 - loss: 1.6206 - val_AUC: 0.9806 - val_accuracy: 0.6170 - val_loss: 1.5749 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "225/225 - 5954s - 26s/step - AUC: 0.9720 - accuracy: 0.5437 - loss: 1.5528 - val_AUC: 0.9814 - val_accuracy: 0.6241 - val_loss: 1.5299 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "225/225 - 2969s - 13s/step - AUC: 0.9758 - accuracy: 0.5517 - loss: 1.4594 - val_AUC: 0.9807 - val_accuracy: 0.6279 - val_loss: 1.4906 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "225/225 - 6171s - 27s/step - AUC: 0.9765 - accuracy: 0.5607 - loss: 1.4118 - val_AUC: 0.9804 - val_accuracy: 0.6316 - val_loss: 1.4543 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "225/225 - 3114s - 14s/step - AUC: 0.9757 - accuracy: 0.5728 - loss: 1.3672 - val_AUC: 0.9806 - val_accuracy: 0.6375 - val_loss: 1.4250 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "225/225 - 6195s - 28s/step - AUC: 0.9772 - accuracy: 0.5759 - loss: 1.3344 - val_AUC: 0.9812 - val_accuracy: 0.6412 - val_loss: 1.4024 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "base_model = EfficientNetB3(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(299, 299, 3)))\n",
    "base_model.trainable = False \n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", \"AUC\"]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\"melhor_modelo.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    verbose=2,\n",
    "    class_weight=class_weights  # <-- aqui!\n",
    ")\n",
    "\n",
    "# Fine-tuning\n",
    "base_model.trainable = True\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", \"AUC\"]\n",
    ")\n",
    "\n",
    "fine_tune_history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2,\n",
    "    class_weight=class_weights  # <-- aqui também!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb518e27",
   "metadata": {},
   "source": [
    "Check some information about scores: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11819c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Carregar o modelo salvo\n",
    "modelo_final = load_model(\"melhor_modelo.keras\")\n",
    "\n",
    "# Previsões\n",
    "y_pred_probs = modelo_final.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Matriz de confusão\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap=\"Blues\")\n",
    "plt.title(\"Matriz de Confusão\")\n",
    "plt.xlabel(\"Previsto\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52244d6e",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7864b06d",
   "metadata": {},
   "source": [
    "### Tentativa errada de tentar aplicar deeplab3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "\n",
    "# Caminho para o diretório com as imagens\n",
    "base_dir = \"/Users/joaosantos/Documents/Mestrado Joao/2 semestre/Deep Learning/rare_speciee\"\n",
    "\n",
    "def load_deeplab_model(input_size=(512, 512)):\n",
    "    \"\"\"Carrega o modelo DeepLabV3+ pré-treinado do TensorFlow (usando DenseNet201 como substituto).\"\"\"\n",
    "    deeplab_model = tf.keras.applications.DenseNet201(input_shape=input_size + (3,), include_top=False, weights='imagenet')\n",
    "    return deeplab_model\n",
    "\n",
    "def segment_image(model, image_path, target_size=(512, 512)):\n",
    "    \"\"\"Aplica o modelo DeepLabV3+ para segmentação de uma imagem.\"\"\"\n",
    "    try:\n",
    "        img = image.load_img(image_path, target_size=target_size)\n",
    "    except (IOError, UnidentifiedImageError) as e:\n",
    "        print(f\"Erro ao carregar a imagem {image_path}: {e}\")\n",
    "        return None  # Retorna None para indicar que a imagem não foi carregada corretamente\n",
    "    \n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0  # Normaliza a imagem\n",
    "    \n",
    "    # Realiza a segmentação\n",
    "    predictions = model.predict(img_array)\n",
    "    \n",
    "    # Convertendo a previsão em uma máscara binária\n",
    "    prediction_mask = np.argmax(predictions[0], axis=-1)\n",
    "    return prediction_mask\n",
    "\n",
    "def apply_mask(image_path, mask, target_size=(64, 64)):\n",
    "    \"\"\"Aplica a máscara segmentada à imagem original.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img_resized = cv2.resize(img, target_size)  # Redimensiona para o tamanho desejado\n",
    "    \n",
    "    # Verificar se a máscara não está vazia\n",
    "    if mask is None or mask.size == 0:\n",
    "        raise ValueError(f\"A máscara gerada está vazia para a imagem {image_path}\")\n",
    "    \n",
    "    mask_resized = cv2.resize(mask.astype(np.uint8), target_size)  # Redimensiona a máscara para o mesmo tamanho da imagem\n",
    "    \n",
    "    # Aplica a máscara (multiplicação elemento a elemento)\n",
    "    masked_image = np.multiply(img_resized, mask_resized[..., None])  # Canal RGB (três canais)\n",
    "    \n",
    "    return masked_image\n",
    "\n",
    "def load_images_with_segmentation(base_dir, model, image_size=(64, 64), is_train=True):\n",
    "    \"\"\"Carrega imagens e aplica a segmentação (DeepLabV3+) nas imagens de treinamento.\"\"\"\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    # Iterar pelas pastas (famílias) no diretório base\n",
    "    labels = os.listdir(base_dir)\n",
    "    labels = [label for label in labels if os.path.isdir(os.path.join(base_dir, label)) and not label.startswith('.')]\n",
    "    \n",
    "    label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    for label in labels:\n",
    "        class_dir = os.path.join(base_dir, label)\n",
    "        \n",
    "        # Ignorar a pasta 'processed', mas carregar as imagens da pasta da classe\n",
    "        if \"processed\" in os.listdir(class_dir):\n",
    "            print(f\"Ignorando a pasta 'processed' na pasta {label}\")\n",
    "        \n",
    "        # Iterar sobre as imagens na pasta da classe (mesmo com a pasta \"processed\" presente)\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            # Ignorar a pasta 'processed'\n",
    "            if img_name == \"processed\":\n",
    "                continue\n",
    "\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"Erro ao carregar a imagem: {img_path}\")\n",
    "                continue  # Ignora a imagem se não for carregada corretamente\n",
    "            else:\n",
    "                print(f\"Imagem carregada com sucesso: {img_path}\")\n",
    "                \n",
    "            # Aplicar segmentação para imagens de treino\n",
    "            if is_train:\n",
    "                mask = segment_image(model, img_path)\n",
    "                if mask is not None:\n",
    "                    img_data = apply_mask(img_path, mask, target_size=image_size)\n",
    "                else:\n",
    "                    continue  # Pula a imagem se não foi carregada corretamente\n",
    "            else:\n",
    "                # Para dados de teste, carregamos a imagem original\n",
    "                img_data = cv2.resize(img, image_size)  # Redimensiona a imagem\n",
    "\n",
    "            X_data.append(img_data)\n",
    "            y_data.append(label_map[label])\n",
    "\n",
    "    # Verificar se algum dado foi carregado\n",
    "    if len(X_data) == 0 or len(y_data) == 0:\n",
    "        print(\"Nenhuma imagem foi carregada. Verifique se o diretório está correto e se as imagens existem.\")\n",
    "    \n",
    "    # Converter para numpy arrays e normalizar\n",
    "    X_data = np.array(X_data, dtype=\"float32\") / 255.0\n",
    "    y_data = np.array(y_data)\n",
    "    \n",
    "    # One-hot encoding dos rótulos\n",
    "    num_classes = len(labels)\n",
    "    y_data = to_categorical(y_data, num_classes=num_classes)\n",
    "    \n",
    "    return X_data, y_data\n",
    "\n",
    "# Carregar o modelo DeepLabV3+ diretamente\n",
    "deeplab_model = load_deeplab_model()\n",
    "\n",
    "# Carregar os dados de treinamento e teste\n",
    "X_data, y_data = load_images_with_segmentation(base_dir, deeplab_model, image_size=(64, 64), is_train=True)\n",
    "\n",
    "# Dividir dados em treinamento e teste (sem segmentação para os dados de teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    ")\n",
    "\n",
    "# Visualizar as dimensões dos dados\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Criar o modelo de classificação\n",
    "num_classes = y_train.shape[1]  # Número de classes\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3),\n",
    "           kernel_regularizer=regularizers.l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "optimizer = Adam(learning_rate=0.00005)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", \"AUC\"]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\"melhor_modelo.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
    "\n",
    "# Treinamento\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "validation_steps = len(X_test) // batch_size\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    verbose=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
