{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "02c40d1e",
      "metadata": {
        "id": "02c40d1e"
      },
      "source": [
        "#  Projeto Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eab3b56",
      "metadata": {
        "id": "0eab3b56"
      },
      "source": [
        "## Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60948300",
      "metadata": {
        "id": "60948300"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, Input\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9fee9c",
      "metadata": {
        "id": "8b9fee9c"
      },
      "source": [
        "## Carregamento do CSV e Verificação de imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oji6bctzD9Ho",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oji6bctzD9Ho",
        "outputId": "c10d7bce-254d-4704-e748-ce7983de37de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "gdrive=True\n",
        "if gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    path = \"/content/drive/MyDrive/DL/DL/\"\n",
        "else:\n",
        "    path = \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63baac55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "63baac55",
        "outputId": "2bbee4df-84c4-4c06-c357-6ec7fb41d7c8"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/DL/metadata.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-71af83880201>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmetadata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/DL/metadata.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmeta_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#base_dir = \"/Users/joaosantos/Documents/Mestrado Joao/2 semestre/Deep Learning/rare_specie\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DL/metadata.csv'"
          ]
        }
      ],
      "source": [
        "metadata_path = \"/content/drive/MyDrive/DL/metadata.csv\"\n",
        "meta_data = pd.read_csv(metadata_path)\n",
        "\n",
        "#base_dir = \"/Users/joaosantos/Documents/Mestrado Joao/2 semestre/Deep Learning/rare_specie\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7PBFjWrfEw3w",
      "metadata": {
        "id": "7PBFjWrfEw3w"
      },
      "outputs": [],
      "source": [
        "if gdrive:\n",
        "  # Extract the files into the \"rare_species\" folder\n",
        "  zip_path = path + '/rare_species_div.zip'\n",
        "  z = zipfile.ZipFile(zip_path)\n",
        "\n",
        "  # Create the extraction folder if it does not exist\n",
        "  os.makedirs(\"rare_species_div\", exist_ok=True)\n",
        "\n",
        "  # Extract the files into the \"rare_species\" folder\n",
        "  z.extractall(\"rare_species_div\")\n",
        "\n",
        "  del z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "InGTkREGEnvU",
      "metadata": {
        "id": "InGTkREGEnvU"
      },
      "outputs": [],
      "source": [
        "base_dir= \"/content/rare_species_div\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7ea26f",
      "metadata": {
        "id": "4e7ea26f"
      },
      "outputs": [],
      "source": [
        "def check_image_exists(image_file):\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        if image_file in files:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8719bd",
      "metadata": {
        "id": "bf8719bd"
      },
      "outputs": [],
      "source": [
        "missing_images = []\n",
        "for index, row in meta_data.iterrows():\n",
        "    image_path = row['file_path']\n",
        "    image_name = os.path.basename(image_path)\n",
        "    if not check_image_exists(image_name):\n",
        "        missing_images.append(image_path)\n",
        "\n",
        "print(f\"Número de imagens em falta: {len(missing_images)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RUswYckgdb2G",
      "metadata": {
        "id": "RUswYckgdb2G"
      },
      "outputs": [],
      "source": [
        "# Função para remover fundo da imagem\n",
        "def remove_background(input_path, output_path):\n",
        "    try:\n",
        "        print(f\"Removendo fundo da imagem: {input_path}\")\n",
        "        # Executa o rembg para remover o fundo\n",
        "        result = subprocess.run(['rembg', 'i', input_path, output_path], check=True, capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"Fundo removido com sucesso de {input_path}. Salvo em {output_path}\")\n",
        "        else:\n",
        "            print(f\"Erro ao processar {input_path}: {result.stderr}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Erro ao processar {input_path}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "388756dc",
      "metadata": {
        "id": "388756dc"
      },
      "source": [
        "## Load Images\n",
        "Carregar imagens visto que cada pasta é uma classe (familia) e dar one hot enconding nas classes , ou seja,\n",
        "X_data são imagens e y_data são rótulos/classes codificados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f19fd03",
      "metadata": {
        "id": "2f19fd03"
      },
      "source": [
        "Resizing usando 224x224, pois medidas mais baixas fazem perder qualidade de imagem e consequentemente pioram a performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QPoxv46McabM",
      "metadata": {
        "id": "QPoxv46McabM"
      },
      "outputs": [],
      "source": [
        "# Definições\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Augmentation para treino\n",
        "train_aug = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Apenas preprocessamento para val\n",
        "val_aug = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BF6TCQR-dKC4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BF6TCQR-dKC4",
        "outputId": "4f2b2ec2-aa31-4522-b39c-be7461008278"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/rare_species_div'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_path=os.getcwd()+ '/rare_species_div'\n",
        "base_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ieaxy7Fdc2ip",
      "metadata": {
        "id": "ieaxy7Fdc2ip"
      },
      "outputs": [],
      "source": [
        "train_dir = base_path+ '/train'\n",
        "val_dir = base_path+ '/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QdAUsptYdx2v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdAUsptYdx2v",
        "outputId": "a008478a-05f4-40e8-9ae3-2852657080e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8627 images belonging to 202 classes.\n",
            "Found 2157 images belonging to 202 classes.\n",
            "X_train batch shape: (32, 224, 224, 3), y_train batch shape: (32, 202)\n",
            "X_val batch shape: (32, 224, 224, 3), y_val batch shape: (32, 202)\n",
            "Num classes: 202\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_dataset = train_aug.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_dataset = val_aug.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Verificar o shape dos batches\n",
        "for X_batch, y_batch in train_dataset:\n",
        "    print(f\"X_train batch shape: {X_batch.shape}, y_train batch shape: {y_batch.shape}\")\n",
        "    break\n",
        "\n",
        "for X_batch, y_batch in val_dataset:\n",
        "    print(f\"X_val batch shape: {X_batch.shape}, y_val batch shape: {y_batch.shape}\")\n",
        "    break\n",
        "\n",
        "# Número de classes\n",
        "num_classes = train_dataset.num_classes\n",
        "print(f\"Num classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w7XeLavROuqL",
      "metadata": {
        "id": "w7XeLavROuqL"
      },
      "source": [
        "## Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LbnEvSxhO1QM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbnEvSxhO1QM",
        "outputId": "69aaa4b2-a3f2-431a-e10c-c4fa849314b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights calculados: {0: np.float64(0.39544371103777043), 1: np.float64(1.9412691269126914), 2: np.float64(0.9932074602809118), 3: np.float64(2.0337105139085336), 4: np.float64(2.0337105139085336), 5: np.float64(0.20434411862238855), 6: np.float64(0.9932074602809118), 7: np.float64(1.9412691269126914), 8: np.float64(1.9412691269126914), 9: np.float64(1.9412691269126914), 10: np.float64(1.9412691269126914), 11: np.float64(1.9412691269126914), 12: np.float64(1.9412691269126914), 13: np.float64(1.9412691269126914), 14: np.float64(1.9412691269126914), 15: np.float64(1.9412691269126914), 16: np.float64(1.9412691269126914), 17: np.float64(0.5024461269656377), 18: np.float64(0.6570449352627571), 19: np.float64(0.9932074602809118), 20: np.float64(1.9412691269126914), 21: np.float64(2.0337105139085336), 22: np.float64(2.0337105139085336), 23: np.float64(0.9932074602809118), 24: np.float64(0.9932074602809118), 25: np.float64(0.33106915342697063), 26: np.float64(0.9932074602809118), 27: np.float64(0.9932074602809118), 28: np.float64(0.9932074602809118), 29: np.float64(2.0337105139085336), 30: np.float64(0.39544371103777043), 31: np.float64(1.9412691269126914), 32: np.float64(2.0337105139085336), 33: np.float64(1.9412691269126914), 34: np.float64(0.9932074602809118), 35: np.float64(2.0337105139085336), 36: np.float64(0.24686659417386825), 37: np.float64(1.9412691269126914), 38: np.float64(0.28283391253032586), 39: np.float64(0.39544371103777043), 40: np.float64(0.9932074602809118), 41: np.float64(1.9412691269126914), 42: np.float64(0.4966037301404559), 43: np.float64(1.9412691269126914), 44: np.float64(1.9412691269126914), 45: np.float64(0.2201439216086557), 46: np.float64(2.0337105139085336), 47: np.float64(0.6570449352627571), 48: np.float64(0.19772185551888521), 49: np.float64(1.9412691269126914), 50: np.float64(1.9412691269126914), 51: np.float64(0.9932074602809118), 52: np.float64(1.0416566046848588), 53: np.float64(0.9932074602809118), 54: np.float64(2.0337105139085336), 55: np.float64(1.0416566046848588), 56: np.float64(1.9412691269126914), 57: np.float64(1.9412691269126914), 58: np.float64(0.6570449352627571), 59: np.float64(0.6570449352627571), 60: np.float64(2.0337105139085336), 61: np.float64(0.9932074602809118), 62: np.float64(1.9412691269126914), 63: np.float64(0.9932074602809118), 64: np.float64(0.9932074602809118), 65: np.float64(1.9412691269126914), 66: np.float64(1.9412691269126914), 67: np.float64(1.9412691269126914), 68: np.float64(0.19772185551888521), 69: np.float64(1.9412691269126914), 70: np.float64(0.39544371103777043), 71: np.float64(1.9412691269126914), 72: np.float64(1.9412691269126914), 73: np.float64(1.9412691269126914), 74: np.float64(0.4966037301404559), 75: np.float64(1.9412691269126914), 76: np.float64(0.33106915342697063), 77: np.float64(0.9932074602809118), 78: np.float64(1.9412691269126914), 79: np.float64(0.9932074602809118), 80: np.float64(1.9412691269126914), 81: np.float64(1.9412691269126914), 82: np.float64(0.9932074602809118), 83: np.float64(0.9932074602809118), 84: np.float64(2.0337105139085336), 85: np.float64(1.9412691269126914), 86: np.float64(0.6570449352627571), 87: np.float64(1.9412691269126914), 88: np.float64(2.0337105139085336), 89: np.float64(1.9412691269126914), 90: np.float64(1.9412691269126914), 91: np.float64(1.9412691269126914), 92: np.float64(1.9412691269126914), 93: np.float64(0.9932074602809118), 94: np.float64(2.0337105139085336), 95: np.float64(1.9412691269126914), 96: np.float64(0.6570449352627571), 97: np.float64(2.0337105139085336), 98: np.float64(0.4966037301404559), 99: np.float64(1.9412691269126914), 100: np.float64(0.9932074602809118), 101: np.float64(0.9932074602809118), 102: np.float64(1.9412691269126914), 103: np.float64(0.39544371103777043), 104: np.float64(1.9412691269126914), 105: np.float64(0.9932074602809118), 106: np.float64(0.9932074602809118), 107: np.float64(1.9412691269126914), 108: np.float64(0.9932074602809118), 109: np.float64(2.0337105139085336), 110: np.float64(2.0337105139085336), 111: np.float64(1.9412691269126914), 112: np.float64(2.0337105139085336), 113: np.float64(1.9412691269126914), 114: np.float64(2.0337105139085336), 115: np.float64(1.9412691269126914), 116: np.float64(0.6570449352627571), 117: np.float64(0.6570449352627571), 118: np.float64(1.9412691269126914), 119: np.float64(1.9412691269126914), 120: np.float64(0.9932074602809118), 121: np.float64(1.9412691269126914), 122: np.float64(1.9412691269126914), 123: np.float64(2.0337105139085336), 124: np.float64(1.9412691269126914), 125: np.float64(1.9412691269126914), 126: np.float64(2.0337105139085336), 127: np.float64(1.9412691269126914), 128: np.float64(1.9412691269126914), 129: np.float64(1.9412691269126914), 130: np.float64(2.0337105139085336), 131: np.float64(1.9412691269126914), 132: np.float64(0.2201439216086557), 133: np.float64(1.9412691269126914), 134: np.float64(1.9412691269126914), 135: np.float64(1.9412691269126914), 136: np.float64(1.9412691269126914), 137: np.float64(1.9412691269126914), 138: np.float64(2.0337105139085336), 139: np.float64(0.6570449352627571), 140: np.float64(0.4966037301404559), 141: np.float64(0.9932074602809118), 142: np.float64(0.9932074602809118), 143: np.float64(0.6570449352627571), 144: np.float64(2.0337105139085336), 145: np.float64(2.0337105139085336), 146: np.float64(0.6570449352627571), 147: np.float64(1.9412691269126914), 148: np.float64(0.9932074602809118), 149: np.float64(1.9412691269126914), 150: np.float64(0.9932074602809118), 151: np.float64(0.2201439216086557), 152: np.float64(2.0337105139085336), 153: np.float64(1.9412691269126914), 154: np.float64(1.9412691269126914), 155: np.float64(0.9932074602809118), 156: np.float64(2.0337105139085336), 157: np.float64(0.6570449352627571), 158: np.float64(2.0337105139085336), 159: np.float64(1.9412691269126914), 160: np.float64(1.9412691269126914), 161: np.float64(0.9932074602809118), 162: np.float64(0.4966037301404559), 163: np.float64(0.6570449352627571), 164: np.float64(2.0337105139085336), 165: np.float64(1.9412691269126914), 166: np.float64(1.9412691269126914), 167: np.float64(1.9412691269126914), 168: np.float64(0.9932074602809118), 169: np.float64(0.9932074602809118), 170: np.float64(0.6570449352627571), 171: np.float64(2.0337105139085336), 172: np.float64(0.6570449352627571), 173: np.float64(1.9412691269126914), 174: np.float64(1.9412691269126914), 175: np.float64(0.9932074602809118), 176: np.float64(1.9412691269126914), 177: np.float64(0.9932074602809118), 178: np.float64(1.9412691269126914), 179: np.float64(0.9932074602809118), 180: np.float64(2.0337105139085336), 181: np.float64(2.0337105139085336), 182: np.float64(2.0337105139085336), 183: np.float64(0.28283391253032586), 184: np.float64(0.4966037301404559), 185: np.float64(2.0337105139085336), 186: np.float64(1.9412691269126914), 187: np.float64(0.6570449352627571), 188: np.float64(0.9932074602809118), 189: np.float64(1.9412691269126914), 190: np.float64(2.0337105139085336), 191: np.float64(2.0337105139085336), 192: np.float64(2.0337105139085336), 193: np.float64(0.9932074602809118), 194: np.float64(0.9932074602809118), 195: np.float64(2.0337105139085336), 196: np.float64(0.9932074602809118), 197: np.float64(0.9932074602809118), 198: np.float64(1.9412691269126914), 199: np.float64(1.9412691269126914), 200: np.float64(0.9932074602809118), 201: np.float64(1.9412691269126914)}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "labels = train_dataset.classes\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(labels),\n",
        "    y=labels\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(\"Class weights calculados:\", class_weight_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef7fd33",
      "metadata": {
        "id": "0ef7fd33"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fV9pgRrPO-aI",
      "metadata": {
        "id": "fV9pgRrPO-aI"
      },
      "source": [
        "### EfficientNetB0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shGkeSTkPEdr",
      "metadata": {
        "id": "shGkeSTkPEdr"
      },
      "source": [
        "Sem Weigths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a566aa55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a566aa55",
        "outputId": "3209fdfc-f08a-4cc1-d997-bde055fc4579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor_2']\n",
            "Received: inputs=Tensor(shape=(None, 224, 224, 3))\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (115600000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 4.40035, saving model to melhor_modelo.keras\n",
            "300/300 - 476s - 2s/step - AUC: 0.6697 - accuracy: 0.0797 - loss: 4.9489 - val_AUC: 0.8178 - val_accuracy: 0.2165 - val_loss: 4.4003 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 2: val_loss improved from 4.40035 to 3.69721, saving model to melhor_modelo.keras\n",
            "300/300 - 382s - 1s/step - AUC: 0.8025 - accuracy: 0.1902 - loss: 4.2049 - val_AUC: 0.9018 - val_accuracy: 0.3062 - val_loss: 3.6972 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 3: val_loss improved from 3.69721 to 3.14595, saving model to melhor_modelo.keras\n",
            "300/300 - 374s - 1s/step - AUC: 0.8749 - accuracy: 0.2650 - loss: 3.6167 - val_AUC: 0.9427 - val_accuracy: 0.3663 - val_loss: 3.1460 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 4: val_loss improved from 3.14595 to 2.71600, saving model to melhor_modelo.keras\n",
            "300/300 - 367s - 1s/step - AUC: 0.9186 - accuracy: 0.3300 - loss: 3.1371 - val_AUC: 0.9609 - val_accuracy: 0.4414 - val_loss: 2.7160 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 5: val_loss improved from 2.71600 to 2.39612, saving model to melhor_modelo.keras\n",
            "300/300 - 383s - 1s/step - AUC: 0.9410 - accuracy: 0.3792 - loss: 2.7790 - val_AUC: 0.9695 - val_accuracy: 0.4894 - val_loss: 2.3961 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 6: val_loss improved from 2.39612 to 2.16463, saving model to melhor_modelo.keras\n",
            "300/300 - 391s - 1s/step - AUC: 0.9514 - accuracy: 0.4185 - loss: 2.5150 - val_AUC: 0.9722 - val_accuracy: 0.5382 - val_loss: 2.1646 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 7: val_loss improved from 2.16463 to 1.99825, saving model to melhor_modelo.keras\n",
            "300/300 - 371s - 1s/step - AUC: 0.9594 - accuracy: 0.4641 - loss: 2.2893 - val_AUC: 0.9740 - val_accuracy: 0.5599 - val_loss: 1.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: val_loss improved from 1.99825 to 1.86825, saving model to melhor_modelo.keras\n",
            "300/300 - 366s - 1s/step - AUC: 0.9623 - accuracy: 0.4945 - loss: 2.1428 - val_AUC: 0.9758 - val_accuracy: 0.5828 - val_loss: 1.8682 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 9: val_loss improved from 1.86825 to 1.77299, saving model to melhor_modelo.keras\n",
            "300/300 - 372s - 1s/step - AUC: 0.9653 - accuracy: 0.5126 - loss: 2.0237 - val_AUC: 0.9757 - val_accuracy: 0.6012 - val_loss: 1.7730 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 10: val_loss improved from 1.77299 to 1.69440, saving model to melhor_modelo.keras\n",
            "300/300 - 374s - 1s/step - AUC: 0.9681 - accuracy: 0.5273 - loss: 1.9359 - val_AUC: 0.9756 - val_accuracy: 0.6120 - val_loss: 1.6944 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n"
          ]
        }
      ],
      "source": [
        "base_model = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(224, 224, 3)))\n",
        "base_model.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "checkpoint = ModelCheckpoint(\"melhor_modelo.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
        "\n",
        "#Treinamento, com base congelada\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=30,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "#Fine-tuning\n",
        "base_model.trainable = True\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "#Treinamento, com base descongelada\n",
        "fine_tune_history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j7pHY-fUPHRU",
      "metadata": {
        "id": "j7pHY-fUPHRU"
      },
      "outputs": [],
      "source": [
        "Com Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0onXcIjDnHkF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0onXcIjDnHkF",
        "outputId": "f8961c74-1ce8-4510-f84f-daf1110a9bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (115600000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 4.92201, saving model to melhor_modelo.keras\n",
            "270/270 - 444s - 2s/step - AUC: 0.5898 - accuracy: 0.0159 - loss: 5.2948 - val_AUC: 0.7569 - val_accuracy: 0.1349 - val_loss: 4.9220 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 2: val_loss improved from 4.92201 to 4.49208, saving model to melhor_modelo.keras\n",
            "270/270 - 357s - 1s/step - AUC: 0.7429 - accuracy: 0.0802 - loss: 4.8475 - val_AUC: 0.8822 - val_accuracy: 0.3018 - val_loss: 4.4921 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 3: val_loss improved from 4.49208 to 3.92398, saving model to melhor_modelo.keras\n",
            "270/270 - 352s - 1s/step - AUC: 0.8491 - accuracy: 0.1844 - loss: 4.3256 - val_AUC: 0.9398 - val_accuracy: 0.3959 - val_loss: 3.9240 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 4: val_loss improved from 3.92398 to 3.35126, saving model to melhor_modelo.keras\n",
            "270/270 - 356s - 1s/step - AUC: 0.9010 - accuracy: 0.2738 - loss: 3.7413 - val_AUC: 0.9572 - val_accuracy: 0.4478 - val_loss: 3.3513 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 5: val_loss improved from 3.35126 to 2.89889, saving model to melhor_modelo.keras\n",
            "270/270 - 378s - 1s/step - AUC: 0.9219 - accuracy: 0.3235 - loss: 3.2547 - val_AUC: 0.9637 - val_accuracy: 0.4835 - val_loss: 2.8989 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 6: val_loss improved from 2.89889 to 2.59452, saving model to melhor_modelo.keras\n",
            "270/270 - 356s - 1s/step - AUC: 0.9386 - accuracy: 0.3772 - loss: 2.8356 - val_AUC: 0.9672 - val_accuracy: 0.5035 - val_loss: 2.5945 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 7: val_loss improved from 2.59452 to 2.38256, saving model to melhor_modelo.keras\n",
            "270/270 - 349s - 1s/step - AUC: 0.9429 - accuracy: 0.3927 - loss: 2.6031 - val_AUC: 0.9683 - val_accuracy: 0.5253 - val_loss: 2.3826 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: val_loss improved from 2.38256 to 2.20653, saving model to melhor_modelo.keras\n",
            "270/270 - 352s - 1s/step - AUC: 0.9476 - accuracy: 0.4284 - loss: 2.3576 - val_AUC: 0.9706 - val_accuracy: 0.5401 - val_loss: 2.2065 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 9: val_loss improved from 2.20653 to 2.08457, saving model to melhor_modelo.keras\n",
            "270/270 - 351s - 1s/step - AUC: 0.9542 - accuracy: 0.4489 - loss: 2.1868 - val_AUC: 0.9715 - val_accuracy: 0.5586 - val_loss: 2.0846 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 10: val_loss improved from 2.08457 to 1.97988, saving model to melhor_modelo.keras\n",
            "270/270 - 376s - 1s/step - AUC: 0.9562 - accuracy: 0.4708 - loss: 2.0574 - val_AUC: 0.9725 - val_accuracy: 0.5693 - val_loss: 1.9799 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 11: val_loss improved from 1.97988 to 1.91003, saving model to melhor_modelo.keras\n",
            "270/270 - 345s - 1s/step - AUC: 0.9587 - accuracy: 0.4806 - loss: 1.9802 - val_AUC: 0.9732 - val_accuracy: 0.5786 - val_loss: 1.9100 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 12: val_loss improved from 1.91003 to 1.83227, saving model to melhor_modelo.keras\n",
            "270/270 - 343s - 1s/step - AUC: 0.9579 - accuracy: 0.4975 - loss: 1.8973 - val_AUC: 0.9741 - val_accuracy: 0.5865 - val_loss: 1.8323 - learning_rate: 1.0000e-04\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 13: val_loss improved from 1.83227 to 1.78477, saving model to melhor_modelo.keras\n",
            "270/270 - 346s - 1s/step - AUC: 0.9618 - accuracy: 0.5078 - loss: 1.8016 - val_AUC: 0.9743 - val_accuracy: 0.5879 - val_loss: 1.7848 - learning_rate: 1.0000e-04\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 14: val_loss improved from 1.78477 to 1.73410, saving model to melhor_modelo.keras\n",
            "270/270 - 377s - 1s/step - AUC: 0.9643 - accuracy: 0.5242 - loss: 1.7423 - val_AUC: 0.9745 - val_accuracy: 0.5962 - val_loss: 1.7341 - learning_rate: 1.0000e-04\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 15: val_loss improved from 1.73410 to 1.68740, saving model to melhor_modelo.keras\n",
            "270/270 - 345s - 1s/step - AUC: 0.9661 - accuracy: 0.5314 - loss: 1.6668 - val_AUC: 0.9758 - val_accuracy: 0.6083 - val_loss: 1.6874 - learning_rate: 1.0000e-04\n",
            "Epoch 16/30\n"
          ]
        }
      ],
      "source": [
        "# === MODELO BASE ===\n",
        "base_model = EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "# === CALLBACKS ===\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "checkpoint = ModelCheckpoint(\"melhor_modelo.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
        "\n",
        "# === TREINO COM BASE CONGELADA ===\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=30,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=2,\n",
        "    class_weight=class_weight_dict\n",
        ")\n",
        "\n",
        "# === FINE-TUNING ===\n",
        "base_model.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "fine_tune_history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=2,\n",
        "    class_weight=class_weight_dict\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MHZzl13FPK1X",
      "metadata": {
        "id": "MHZzl13FPK1X"
      },
      "source": [
        "### ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k8_xibsdN6Kb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8_xibsdN6Kb",
        "outputId": "de827bf2-a6c5-4014-b436-51caca1825bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (115600000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 5.09961, saving model to melhor_modelo.keras\n",
            "270/270 - 402s - 1s/step - AUC: 0.5544 - accuracy: 0.0131 - loss: 5.6980 - val_AUC: 0.6552 - val_accuracy: 0.0487 - val_loss: 5.0996 - learning_rate: 1.0000e-04\n",
            "Epoch 2/25\n",
            "\n",
            "Epoch 2: val_loss improved from 5.09961 to 4.87766, saving model to melhor_modelo.keras\n",
            "270/270 - 381s - 1s/step - AUC: 0.6319 - accuracy: 0.0275 - loss: 5.1583 - val_AUC: 0.7397 - val_accuracy: 0.1080 - val_loss: 4.8777 - learning_rate: 1.0000e-04\n",
            "Epoch 3/25\n",
            "\n",
            "Epoch 3: val_loss improved from 4.87766 to 4.45475, saving model to melhor_modelo.keras\n",
            "270/270 - 331s - 1s/step - AUC: 0.6917 - accuracy: 0.0562 - loss: 4.8984 - val_AUC: 0.8290 - val_accuracy: 0.1734 - val_loss: 4.4548 - learning_rate: 1.0000e-04\n",
            "Epoch 4/25\n",
            "\n",
            "Epoch 4: val_loss improved from 4.45475 to 3.89858, saving model to melhor_modelo.keras\n",
            "270/270 - 333s - 1s/step - AUC: 0.7575 - accuracy: 0.0982 - loss: 4.5311 - val_AUC: 0.8790 - val_accuracy: 0.2272 - val_loss: 3.8986 - learning_rate: 1.0000e-04\n",
            "Epoch 5/25\n",
            "\n",
            "Epoch 5: val_loss improved from 3.89858 to 3.44412, saving model to melhor_modelo.keras\n",
            "270/270 - 367s - 1s/step - AUC: 0.8076 - accuracy: 0.1450 - loss: 4.1242 - val_AUC: 0.9033 - val_accuracy: 0.2758 - val_loss: 3.4441 - learning_rate: 1.0000e-04\n",
            "Epoch 6/25\n",
            "\n",
            "Epoch 6: val_loss improved from 3.44412 to 3.14149, saving model to melhor_modelo.keras\n",
            "270/270 - 337s - 1s/step - AUC: 0.8350 - accuracy: 0.1858 - loss: 3.7683 - val_AUC: 0.9180 - val_accuracy: 0.3250 - val_loss: 3.1415 - learning_rate: 1.0000e-04\n",
            "Epoch 7/25\n",
            "\n",
            "Epoch 7: val_loss improved from 3.14149 to 2.94265, saving model to melhor_modelo.keras\n",
            "270/270 - 332s - 1s/step - AUC: 0.8535 - accuracy: 0.2090 - loss: 3.5278 - val_AUC: 0.9289 - val_accuracy: 0.3408 - val_loss: 2.9427 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "\n",
            "Epoch 8: val_loss improved from 2.94265 to 2.77667, saving model to melhor_modelo.keras\n",
            "270/270 - 329s - 1s/step - AUC: 0.8701 - accuracy: 0.2488 - loss: 3.2959 - val_AUC: 0.9345 - val_accuracy: 0.3611 - val_loss: 2.7767 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "\n",
            "Epoch 9: val_loss improved from 2.77667 to 2.63499, saving model to melhor_modelo.keras\n",
            "270/270 - 336s - 1s/step - AUC: 0.8813 - accuracy: 0.2641 - loss: 3.1049 - val_AUC: 0.9383 - val_accuracy: 0.3885 - val_loss: 2.6350 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "\n",
            "Epoch 10: val_loss improved from 2.63499 to 2.57486, saving model to melhor_modelo.keras\n",
            "270/270 - 328s - 1s/step - AUC: 0.8903 - accuracy: 0.2906 - loss: 2.9649 - val_AUC: 0.9414 - val_accuracy: 0.3941 - val_loss: 2.5749 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "\n",
            "Epoch 11: val_loss improved from 2.57486 to 2.46831, saving model to melhor_modelo.keras\n",
            "270/270 - 331s - 1s/step - AUC: 0.9002 - accuracy: 0.3057 - loss: 2.8586 - val_AUC: 0.9446 - val_accuracy: 0.4145 - val_loss: 2.4683 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "\n",
            "Epoch 12: val_loss improved from 2.46831 to 2.42187, saving model to melhor_modelo.keras\n",
            "270/270 - 338s - 1s/step - AUC: 0.9056 - accuracy: 0.3212 - loss: 2.7275 - val_AUC: 0.9442 - val_accuracy: 0.4251 - val_loss: 2.4219 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "\n",
            "Epoch 13: val_loss improved from 2.42187 to 2.34886, saving model to melhor_modelo.keras\n",
            "270/270 - 334s - 1s/step - AUC: 0.9111 - accuracy: 0.3351 - loss: 2.6204 - val_AUC: 0.9488 - val_accuracy: 0.4256 - val_loss: 2.3489 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "\n",
            "Epoch 14: val_loss improved from 2.34886 to 2.27551, saving model to melhor_modelo.keras\n",
            "270/270 - 334s - 1s/step - AUC: 0.9180 - accuracy: 0.3559 - loss: 2.4800 - val_AUC: 0.9510 - val_accuracy: 0.4423 - val_loss: 2.2755 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "\n",
            "Epoch 15: val_loss improved from 2.27551 to 2.23601, saving model to melhor_modelo.keras\n",
            "270/270 - 367s - 1s/step - AUC: 0.9195 - accuracy: 0.3659 - loss: 2.4540 - val_AUC: 0.9523 - val_accuracy: 0.4543 - val_loss: 2.2360 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "\n",
            "Epoch 16: val_loss improved from 2.23601 to 2.19261, saving model to melhor_modelo.keras\n",
            "270/270 - 368s - 1s/step - AUC: 0.9233 - accuracy: 0.3828 - loss: 2.3414 - val_AUC: 0.9515 - val_accuracy: 0.4655 - val_loss: 2.1926 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "\n",
            "Epoch 17: val_loss improved from 2.19261 to 2.18738, saving model to melhor_modelo.keras\n",
            "270/270 - 344s - 1s/step - AUC: 0.9279 - accuracy: 0.3869 - loss: 2.2973 - val_AUC: 0.9516 - val_accuracy: 0.4585 - val_loss: 2.1874 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "\n",
            "Epoch 18: val_loss improved from 2.18738 to 2.14035, saving model to melhor_modelo.keras\n",
            "270/270 - 331s - 1s/step - AUC: 0.9291 - accuracy: 0.4050 - loss: 2.2018 - val_AUC: 0.9527 - val_accuracy: 0.4636 - val_loss: 2.1404 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "\n",
            "Epoch 19: val_loss improved from 2.14035 to 2.12312, saving model to melhor_modelo.keras\n",
            "270/270 - 332s - 1s/step - AUC: 0.9364 - accuracy: 0.4094 - loss: 2.1342 - val_AUC: 0.9529 - val_accuracy: 0.4733 - val_loss: 2.1231 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "\n",
            "Epoch 20: val_loss improved from 2.12312 to 2.08822, saving model to melhor_modelo.keras\n",
            "270/270 - 334s - 1s/step - AUC: 0.9345 - accuracy: 0.4274 - loss: 2.0830 - val_AUC: 0.9560 - val_accuracy: 0.4761 - val_loss: 2.0882 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "\n",
            "Epoch 21: val_loss improved from 2.08822 to 2.06054, saving model to melhor_modelo.keras\n",
            "270/270 - 335s - 1s/step - AUC: 0.9399 - accuracy: 0.4271 - loss: 2.0253 - val_AUC: 0.9546 - val_accuracy: 0.4840 - val_loss: 2.0605 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "\n",
            "Epoch 22: val_loss improved from 2.06054 to 2.03528, saving model to melhor_modelo.keras\n",
            "270/270 - 332s - 1s/step - AUC: 0.9432 - accuracy: 0.4478 - loss: 1.9586 - val_AUC: 0.9569 - val_accuracy: 0.4905 - val_loss: 2.0353 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "\n",
            "Epoch 23: val_loss improved from 2.03528 to 2.02301, saving model to melhor_modelo.keras\n",
            "270/270 - 381s - 1s/step - AUC: 0.9428 - accuracy: 0.4413 - loss: 1.9493 - val_AUC: 0.9537 - val_accuracy: 0.4979 - val_loss: 2.0230 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "\n",
            "Epoch 24: val_loss improved from 2.02301 to 2.00468, saving model to melhor_modelo.keras\n",
            "270/270 - 327s - 1s/step - AUC: 0.9445 - accuracy: 0.4542 - loss: 1.8996 - val_AUC: 0.9544 - val_accuracy: 0.4924 - val_loss: 2.0047 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "\n",
            "Epoch 25: val_loss improved from 2.00468 to 1.96441, saving model to melhor_modelo.keras\n",
            "270/270 - 331s - 1s/step - AUC: 0.9476 - accuracy: 0.4676 - loss: 1.8238 - val_AUC: 0.9559 - val_accuracy: 0.5058 - val_loss: 1.9644 - learning_rate: 1.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 25.\n",
            "Epoch 1/10\n",
            "270/270 - 444s - 2s/step - AUC: 0.9462 - accuracy: 0.4307 - loss: 2.0857 - val_AUC: 0.9541 - val_accuracy: 0.5216 - val_loss: 1.8768 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "270/270 - 345s - 1s/step - AUC: 0.9595 - accuracy: 0.4814 - loss: 1.7634 - val_AUC: 0.9577 - val_accuracy: 0.5410 - val_loss: 1.8321 - learning_rate: 1.0000e-05\n",
            "Epoch 3/10\n",
            "270/270 - 343s - 1s/step - AUC: 0.9643 - accuracy: 0.5101 - loss: 1.5906 - val_AUC: 0.9590 - val_accuracy: 0.5531 - val_loss: 1.7737 - learning_rate: 1.0000e-05\n",
            "Epoch 4/10\n",
            "270/270 - 342s - 1s/step - AUC: 0.9671 - accuracy: 0.5294 - loss: 1.4706 - val_AUC: 0.9586 - val_accuracy: 0.5591 - val_loss: 1.7441 - learning_rate: 1.0000e-05\n",
            "Epoch 5/10\n",
            "270/270 - 346s - 1s/step - AUC: 0.9690 - accuracy: 0.5525 - loss: 1.3804 - val_AUC: 0.9590 - val_accuracy: 0.5716 - val_loss: 1.7071 - learning_rate: 1.0000e-05\n",
            "Epoch 6/10\n",
            "270/270 - 383s - 1s/step - AUC: 0.9704 - accuracy: 0.5653 - loss: 1.3179 - val_AUC: 0.9597 - val_accuracy: 0.5739 - val_loss: 1.6881 - learning_rate: 1.0000e-05\n",
            "Epoch 7/10\n",
            "270/270 - 341s - 1s/step - AUC: 0.9737 - accuracy: 0.5897 - loss: 1.2112 - val_AUC: 0.9610 - val_accuracy: 0.5818 - val_loss: 1.6742 - learning_rate: 1.0000e-05\n",
            "Epoch 8/10\n",
            "270/270 - 354s - 1s/step - AUC: 0.9734 - accuracy: 0.5876 - loss: 1.1887 - val_AUC: 0.9616 - val_accuracy: 0.5823 - val_loss: 1.6410 - learning_rate: 1.0000e-05\n",
            "Epoch 9/10\n",
            "270/270 - 346s - 1s/step - AUC: 0.9760 - accuracy: 0.6121 - loss: 1.1131 - val_AUC: 0.9597 - val_accuracy: 0.5869 - val_loss: 1.6263 - learning_rate: 1.0000e-05\n",
            "Epoch 10/10\n",
            "270/270 - 345s - 1s/step - AUC: 0.9768 - accuracy: 0.6229 - loss: 1.0588 - val_AUC: 0.9606 - val_accuracy: 0.5925 - val_loss: 1.6177 - learning_rate: 1.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# === MODELO BASE (ResNet50) ===\n",
        "base_model_rn = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
        "base_model_rn.trainable = False  # Congelar as camadas convolucionais\n",
        "\n",
        "# Adicionar camadas de classificação\n",
        "x = base_model_rn.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)  # Dropout para evitar overfitting\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "outputs = Dense(num_classes, activation=\"softmax\")(x)  # Saída com softmax\n",
        "\n",
        "# Criar o modelo final\n",
        "model_rn = Model(inputs=base_model_rn.input, outputs=outputs)\n",
        "\n",
        "# Compilar o modelo\n",
        "model_rn.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),  # Definir uma taxa de aprendizado inicial\n",
        "    loss=\"categorical_crossentropy\",  # Função de perda para múltiplas classes\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "# === CALLBACKS ===\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "checkpoint = ModelCheckpoint(\"melhor_modelo.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
        "\n",
        "# === TREINAMENTO COM BASE CONGELADA ===\n",
        "history = model_rn.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=25,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=2,\n",
        "    class_weight=class_weight_dict  # Considerando os pesos das classes\n",
        ")\n",
        "\n",
        "# === FINE-TUNING ===\n",
        "base_model_rn.trainable = True  # Descongelar as camadas convolucionais para fine-tuning\n",
        "\n",
        "# Recompilar o modelo com uma taxa de aprendizado menor para o fine-tuning\n",
        "model_rn.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),  # Reduzir a taxa de aprendizado para fine-tuning\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "# Fine-tuning - treinamento com as camadas superiores descongeladas\n",
        "fine_tune_history = model_rn.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=2,\n",
        "    class_weight=class_weight_dict  # Considerando os pesos das classes\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fPzjSYtkn6xb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPzjSYtkn6xb",
        "outputId": "940b1fb2-72cb-49d6-c759-a3d58025edb4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (115600000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "270/270 - 511s - 2s/step - AUC: 0.5670 - accuracy: 0.0195 - loss: 5.4866 - val_AUC: 0.7330 - val_accuracy: 0.0872 - val_loss: 4.7522 - learning_rate: 5.0000e-05\n",
            "Epoch 2/10\n",
            "270/270 - 388s - 1s/step - AUC: 0.7082 - accuracy: 0.0811 - loss: 4.7205 - val_AUC: 0.8612 - val_accuracy: 0.2137 - val_loss: 3.8909 - learning_rate: 5.0000e-05\n",
            "Epoch 3/10\n",
            "270/270 - 379s - 1s/step - AUC: 0.8151 - accuracy: 0.1713 - loss: 3.8756 - val_AUC: 0.9097 - val_accuracy: 0.3162 - val_loss: 3.1920 - learning_rate: 5.0000e-05\n",
            "Epoch 4/10\n",
            "270/270 - 381s - 1s/step - AUC: 0.8773 - accuracy: 0.2612 - loss: 3.1840 - val_AUC: 0.9324 - val_accuracy: 0.3839 - val_loss: 2.6796 - learning_rate: 5.0000e-05\n",
            "Epoch 5/10\n",
            "270/270 - 372s - 1s/step - AUC: 0.9056 - accuracy: 0.3382 - loss: 2.6536 - val_AUC: 0.9431 - val_accuracy: 0.4182 - val_loss: 2.5070 - learning_rate: 5.0000e-05\n",
            "Epoch 6/10\n",
            "270/270 - 375s - 1s/step - AUC: 0.9301 - accuracy: 0.4022 - loss: 2.2306 - val_AUC: 0.9432 - val_accuracy: 0.4622 - val_loss: 2.2844 - learning_rate: 5.0000e-05\n",
            "Epoch 7/10\n",
            "270/270 - 372s - 1s/step - AUC: 0.9416 - accuracy: 0.4654 - loss: 1.8839 - val_AUC: 0.9450 - val_accuracy: 0.4886 - val_loss: 2.1691 - learning_rate: 5.0000e-05\n",
            "Epoch 8/10\n",
            "270/270 - 378s - 1s/step - AUC: 0.9559 - accuracy: 0.5223 - loss: 1.5882 - val_AUC: 0.9455 - val_accuracy: 0.4947 - val_loss: 2.1013 - learning_rate: 5.0000e-05\n",
            "Epoch 9/10\n",
            "270/270 - 383s - 1s/step - AUC: 0.9632 - accuracy: 0.5703 - loss: 1.3494 - val_AUC: 0.9496 - val_accuracy: 0.5183 - val_loss: 2.0000 - learning_rate: 5.0000e-05\n",
            "Epoch 10/10\n",
            "270/270 - 381s - 1s/step - AUC: 0.9690 - accuracy: 0.6058 - loss: 1.1844 - val_AUC: 0.9453 - val_accuracy: 0.5461 - val_loss: 1.9325 - learning_rate: 5.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# === MODELO BASE (ResNet50) ===\n",
        "base_model_rn = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
        "base_model_rn.trainable = False  # Congelar as camadas convolucionais\n",
        "\n",
        "# Adicionar camadas de classificação\n",
        "x = base_model_rn.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)  # Dropout para evitar overfitting\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(num_classes, activation=\"softmax\")(x)  # Saída com softmax\n",
        "\n",
        "# Criar o modelo final\n",
        "model_rn = Model(inputs=base_model_rn.input, outputs=outputs)\n",
        "\n",
        "# Compilar o modelo\n",
        "model_rn.compile(\n",
        "    optimizer=Adam(learning_rate=5e-4),  # Definir uma taxa de aprendizado inicial\n",
        "    loss=\"categorical_crossentropy\",  # Função de perda para múltiplas classes\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "# === CALLBACKS ===\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "checkpoint = ModelCheckpoint(\"melhor_modelo.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
        "\n",
        "\n",
        "# === FINE-TUNING ===\n",
        "base_model_rn.trainable = True  # Descongelar as camadas convolucionais para fine-tuning\n",
        "\n",
        "# Recompilar o modelo com uma taxa de aprendizado menor para o fine-tuning\n",
        "model_rn.compile(\n",
        "    optimizer=Adam(learning_rate=5e-5),  # Reduzir a taxa de aprendizado para fine-tuning\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "# Fine-tuning - treinamento com as camadas superiores descongeladas\n",
        "fine_tune_history = model_rn.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=2,\n",
        "    class_weight=class_weight_dict  # Considerando os pesos das classes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gb3tyfj_cs7k",
      "metadata": {
        "id": "Gb3tyfj_cs7k"
      },
      "source": [
        "### DenseNet121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32OQcpELcqgu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "32OQcpELcqgu",
        "outputId": "814aaad5-c622-4651-c644-0098a94ac8e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor']\n",
            "Received: inputs=Tensor(shape=(None, 224, 224, 3))\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (115600000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 5.31984, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 448s - 2s/step - AUC: 0.5247 - accuracy: 0.0115 - loss: 11.5109 - val_AUC: 0.5950 - val_accuracy: 0.0343 - val_loss: 5.3198 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 2: val_loss improved from 5.31984 to 5.26354, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 380s - 1s/step - AUC: 0.5489 - accuracy: 0.0165 - loss: 6.8928 - val_AUC: 0.5648 - val_accuracy: 0.0301 - val_loss: 5.2635 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 3: val_loss did not improve from 5.26354\n",
            "270/270 - 363s - 1s/step - AUC: 0.5654 - accuracy: 0.0170 - loss: 5.8307 - val_AUC: 0.5923 - val_accuracy: 0.0343 - val_loss: 5.2769 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 4: val_loss did not improve from 5.26354\n",
            "270/270 - 362s - 1s/step - AUC: 0.5677 - accuracy: 0.0206 - loss: 5.5234 - val_AUC: 0.6163 - val_accuracy: 0.0362 - val_loss: 5.2765 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\n",
            "Epoch 5: val_loss did not improve from 5.26354\n",
            "270/270 - 363s - 1s/step - AUC: 0.5738 - accuracy: 0.0227 - loss: 5.3865 - val_AUC: 0.6262 - val_accuracy: 0.0403 - val_loss: 5.2723 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 6: val_loss did not improve from 5.26354\n",
            "270/270 - 364s - 1s/step - AUC: 0.5838 - accuracy: 0.0239 - loss: 5.3288 - val_AUC: 0.6246 - val_accuracy: 0.0399 - val_loss: 5.2666 - learning_rate: 5.0000e-05\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 7: val_loss improved from 5.26354 to 5.25658, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 369s - 1s/step - AUC: 0.5864 - accuracy: 0.0282 - loss: 5.3052 - val_AUC: 0.6251 - val_accuracy: 0.0468 - val_loss: 5.2566 - learning_rate: 5.0000e-05\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: val_loss improved from 5.25658 to 5.24121, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 363s - 1s/step - AUC: 0.5989 - accuracy: 0.0305 - loss: 5.2690 - val_AUC: 0.6289 - val_accuracy: 0.0533 - val_loss: 5.2412 - learning_rate: 5.0000e-05\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 9: val_loss improved from 5.24121 to 5.22622, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 363s - 1s/step - AUC: 0.6023 - accuracy: 0.0338 - loss: 5.2547 - val_AUC: 0.6314 - val_accuracy: 0.0580 - val_loss: 5.2262 - learning_rate: 5.0000e-05\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 10: val_loss improved from 5.22622 to 5.21309, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 376s - 1s/step - AUC: 0.6132 - accuracy: 0.0378 - loss: 5.2219 - val_AUC: 0.6336 - val_accuracy: 0.0621 - val_loss: 5.2131 - learning_rate: 5.0000e-05\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 11: val_loss improved from 5.21309 to 5.18242, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 407s - 2s/step - AUC: 0.6198 - accuracy: 0.0376 - loss: 5.1879 - val_AUC: 0.6388 - val_accuracy: 0.0617 - val_loss: 5.1824 - learning_rate: 5.0000e-05\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 12: val_loss improved from 5.18242 to 5.16102, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 371s - 1s/step - AUC: 0.6296 - accuracy: 0.0447 - loss: 5.1574 - val_AUC: 0.6460 - val_accuracy: 0.0593 - val_loss: 5.1610 - learning_rate: 5.0000e-05\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 13: val_loss improved from 5.16102 to 5.13639, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 359s - 1s/step - AUC: 0.6371 - accuracy: 0.0479 - loss: 5.1359 - val_AUC: 0.6508 - val_accuracy: 0.0617 - val_loss: 5.1364 - learning_rate: 5.0000e-05\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 14: val_loss improved from 5.13639 to 5.11852, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 362s - 1s/step - AUC: 0.6387 - accuracy: 0.0468 - loss: 5.1268 - val_AUC: 0.6534 - val_accuracy: 0.0649 - val_loss: 5.1185 - learning_rate: 5.0000e-05\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 15: val_loss improved from 5.11852 to 5.09850, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 362s - 1s/step - AUC: 0.6432 - accuracy: 0.0500 - loss: 5.1029 - val_AUC: 0.6643 - val_accuracy: 0.0695 - val_loss: 5.0985 - learning_rate: 5.0000e-05\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 16: val_loss improved from 5.09850 to 5.05076, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 353s - 1s/step - AUC: 0.6552 - accuracy: 0.0529 - loss: 5.0693 - val_AUC: 0.6710 - val_accuracy: 0.0705 - val_loss: 5.0508 - learning_rate: 5.0000e-05\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 17: val_loss improved from 5.05076 to 5.02859, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 360s - 1s/step - AUC: 0.6576 - accuracy: 0.0516 - loss: 5.0554 - val_AUC: 0.6768 - val_accuracy: 0.0737 - val_loss: 5.0286 - learning_rate: 5.0000e-05\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 18: val_loss improved from 5.02859 to 5.01151, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 364s - 1s/step - AUC: 0.6662 - accuracy: 0.0547 - loss: 5.0357 - val_AUC: 0.6825 - val_accuracy: 0.0770 - val_loss: 5.0115 - learning_rate: 5.0000e-05\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 19: val_loss improved from 5.01151 to 4.97435, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 360s - 1s/step - AUC: 0.6653 - accuracy: 0.0603 - loss: 5.0117 - val_AUC: 0.6918 - val_accuracy: 0.0834 - val_loss: 4.9744 - learning_rate: 5.0000e-05\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 20: val_loss improved from 4.97435 to 4.95996, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 361s - 1s/step - AUC: 0.6760 - accuracy: 0.0627 - loss: 4.9936 - val_AUC: 0.6992 - val_accuracy: 0.0862 - val_loss: 4.9600 - learning_rate: 5.0000e-05\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 21: val_loss improved from 4.95996 to 4.93379, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 364s - 1s/step - AUC: 0.6831 - accuracy: 0.0661 - loss: 4.9610 - val_AUC: 0.7032 - val_accuracy: 0.0899 - val_loss: 4.9338 - learning_rate: 5.0000e-05\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 22: val_loss improved from 4.93379 to 4.89585, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 361s - 1s/step - AUC: 0.6857 - accuracy: 0.0625 - loss: 4.9426 - val_AUC: 0.7092 - val_accuracy: 0.0885 - val_loss: 4.8958 - learning_rate: 5.0000e-05\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 23: val_loss improved from 4.89585 to 4.87889, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 358s - 1s/step - AUC: 0.6897 - accuracy: 0.0672 - loss: 4.9212 - val_AUC: 0.7150 - val_accuracy: 0.0909 - val_loss: 4.8789 - learning_rate: 5.0000e-05\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 24: val_loss improved from 4.87889 to 4.85039, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 361s - 1s/step - AUC: 0.6960 - accuracy: 0.0692 - loss: 4.9034 - val_AUC: 0.7231 - val_accuracy: 0.0946 - val_loss: 4.8504 - learning_rate: 5.0000e-05\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 25: val_loss improved from 4.85039 to 4.83888, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 391s - 1s/step - AUC: 0.6972 - accuracy: 0.0672 - loss: 4.8960 - val_AUC: 0.7266 - val_accuracy: 0.0909 - val_loss: 4.8389 - learning_rate: 5.0000e-05\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 26: val_loss improved from 4.83888 to 4.80940, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 388s - 1s/step - AUC: 0.7025 - accuracy: 0.0657 - loss: 4.8698 - val_AUC: 0.7311 - val_accuracy: 0.0936 - val_loss: 4.8094 - learning_rate: 5.0000e-05\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 27: val_loss improved from 4.80940 to 4.79816, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 360s - 1s/step - AUC: 0.7041 - accuracy: 0.0692 - loss: 4.8604 - val_AUC: 0.7387 - val_accuracy: 0.0969 - val_loss: 4.7982 - learning_rate: 5.0000e-05\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 28: val_loss improved from 4.79816 to 4.77330, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 355s - 1s/step - AUC: 0.7099 - accuracy: 0.0715 - loss: 4.8345 - val_AUC: 0.7416 - val_accuracy: 0.0960 - val_loss: 4.7733 - learning_rate: 5.0000e-05\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 29: val_loss improved from 4.77330 to 4.73635, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 358s - 1s/step - AUC: 0.7133 - accuracy: 0.0766 - loss: 4.8191 - val_AUC: 0.7476 - val_accuracy: 0.1006 - val_loss: 4.7363 - learning_rate: 5.0000e-05\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 30: val_loss improved from 4.73635 to 4.73013, saving model to melhor_modelo_densenet.keras\n",
            "270/270 - 352s - 1s/step - AUC: 0.7177 - accuracy: 0.0758 - loss: 4.8093 - val_AUC: 0.7478 - val_accuracy: 0.1015 - val_loss: 4.7301 - learning_rate: 5.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 30.\n",
            "Epoch 1/10\n",
            "270/270 - 640s - 2s/step - AUC: 0.6518 - accuracy: 0.0319 - loss: 5.0933 - val_AUC: 0.6972 - val_accuracy: 0.0677 - val_loss: 5.0046 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "270/270 - 370s - 1s/step - AUC: 0.6997 - accuracy: 0.0627 - loss: 4.9145 - val_AUC: 0.7459 - val_accuracy: 0.1080 - val_loss: 4.7745 - learning_rate: 1.0000e-05\n",
            "Epoch 3/10\n",
            "270/270 - 380s - 1s/step - AUC: 0.7289 - accuracy: 0.0871 - loss: 4.7194 - val_AUC: 0.7709 - val_accuracy: 0.1201 - val_loss: 4.5413 - learning_rate: 1.0000e-05\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e908ac6e1a15>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Treinamento com base descongelada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m fine_tune_history = model.fit(\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Base model: DenseNet121\n",
        "base_model = DenseNet121(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(224, 224, 3)))\n",
        "base_model.trainable = False  # Fase 1: feature extractor\n",
        "\n",
        "# Classifier head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Compilar o modelo (fase 1)\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "checkpoint = ModelCheckpoint(\"melhor_modelo_densenet.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
        "\n",
        "# Treinamento com base congelada\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=30,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Fine-tuning: descongelar a base\n",
        "base_model.trainable = True\n",
        "\n",
        "# Compilar novamente com learning rate menor\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "# Treinamento com base descongelada\n",
        "fine_tune_history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb518e27",
      "metadata": {
        "id": "cb518e27"
      },
      "source": [
        "Check some information about scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11819c2",
      "metadata": {
        "id": "f11819c2"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.models import load_model\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Carregar o modelo salvo\n",
        "# modelo_final = load_model(\"melhor_modelo.keras\")\n",
        "\n",
        "# # Previsões\n",
        "# y_pred_probs = modelo_final.predict(X_val)\n",
        "# y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "# y_true = np.argmax(y_val, axis=1)\n",
        "\n",
        "# # Classification report\n",
        "# print(\"Classification Report:\")\n",
        "# print(classification_report(y_true, y_pred))\n",
        "\n",
        "# # Matriz de confusão\n",
        "# plt.figure(figsize=(8,6))\n",
        "# sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap=\"Blues\")\n",
        "# plt.title(\"Matriz de Confusão\")\n",
        "# plt.xlabel(\"Previsto\")\n",
        "# plt.ylabel(\"Real\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2T5sp4kt1tzD",
      "metadata": {
        "id": "2T5sp4kt1tzD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Guardar histórico do treino ---\n",
        "\n",
        "def save_history(history, fine_tune_history, filename='training_history.csv'):\n",
        "    # Junta os históricos\n",
        "    def extract_data(hist):\n",
        "        return pd.DataFrame(hist.history)\n",
        "\n",
        "    combined_history = pd.concat([extract_data(history), extract_data(fine_tune_history)], ignore_index=True)\n",
        "    combined_history.to_csv(filename, index=False)\n",
        "    print(f\"Histórico guardado em {filename}\")\n",
        "\n",
        "# --- 2. Plotar curvas de treino ---\n",
        "\n",
        "def plot_history(history, fine_tune_history):\n",
        "    def merge(hist1, hist2, metric):\n",
        "        return hist1.history[metric] + hist2.history[metric]\n",
        "\n",
        "    epochs_range = range(len(history.history['loss']) + len(fine_tune_history.history['loss']))\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs_range, merge(history, fine_tune_history, 'loss'), label='Treino')\n",
        "    plt.plot(epochs_range, merge(history, fine_tune_history, 'val_loss'), label='Validação')\n",
        "    plt.title('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs_range, merge(history, fine_tune_history, 'accuracy'), label='Treino')\n",
        "    plt.plot(epochs_range, merge(history, fine_tune_history, 'val_accuracy'), label='Validação')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # AUC\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs_range, merge(history, fine_tune_history, 'AUC'), label='Treino')\n",
        "    plt.plot(epochs_range, merge(history, fine_tune_history, 'val_AUC'), label='Validação')\n",
        "    plt.title('AUC')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- 3. Matriz de confusão ---\n",
        "\n",
        "def plot_confusion_matrix(model, dataset, class_names):\n",
        "    # Gerar previsões\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for images, labels in dataset:\n",
        "        preds = model.predict(images)\n",
        "        y_true.extend(np.argmax(labels, axis=1))\n",
        "        y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "        if len(y_true) >= dataset.samples:\n",
        "            break\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Matriz de Confusão')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nRelatório de Classificação:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# --- 4. Guardar modelo final ---\n",
        "\n",
        "def save_model(model, filename='modelo_final.keras'):\n",
        "    model.save(filename)\n",
        "    print(f\"Modelo final guardado em {filename}\")\n",
        "\n",
        "# =========================================================\n",
        "# 🚀 Usa as funções depois de treinares:\n",
        "\n",
        "# Guardar histórico\n",
        "save_history(history, fine_tune_history)\n",
        "\n",
        "# Plotar as curvas\n",
        "plot_history(history, fine_tune_history)\n",
        "\n",
        "# Matriz de confusão\n",
        "plot_confusion_matrix(model, val_dataset, list(train_dataset.class_indices.keys()))\n",
        "\n",
        "# Guardar o modelo final\n",
        "save_model(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52244d6e",
      "metadata": {
        "id": "52244d6e"
      },
      "source": [
        "------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7864b06d",
      "metadata": {
        "id": "7864b06d"
      },
      "source": [
        "### Com gaussian blur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2VSkUQwCBtBv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2VSkUQwCBtBv",
        "outputId": "fae3710a-17bf-4ccb-b90c-100607305a3c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/rare_species_div'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_path=os.getcwd()+ '/rare_species_div'\n",
        "base_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vjWH0PG1Bqi-",
      "metadata": {
        "id": "vjWH0PG1Bqi-"
      },
      "outputs": [],
      "source": [
        "train_dir = base_path+ '/train'\n",
        "val_dir = base_path+ '/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fCWao16xBerj",
      "metadata": {
        "id": "fCWao16xBerj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "def custom_preprocessing(image):\n",
        "    # Converte para uint8 se necessário\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Aplica Gaussian Blur (kernel 3x3, pode ajustar para 5x5)\n",
        "    image = cv2.GaussianBlur(image, (3, 3), 0)\n",
        "\n",
        "    # Converte de volta para float32\n",
        "    image = image.astype(np.float32)\n",
        "\n",
        "    # Aplica preprocess_input (ex. para EfficientNet)\n",
        "    return preprocess_input(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2TxtYSOyBl8S",
      "metadata": {
        "id": "2TxtYSOyBl8S"
      },
      "outputs": [],
      "source": [
        "train_aug = ImageDataGenerator(\n",
        "    preprocessing_function=custom_preprocessing,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_aug = ImageDataGenerator(\n",
        "    preprocessing_function=custom_preprocessing\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n9vV-OiQD9nS",
      "metadata": {
        "id": "n9vV-OiQD9nS"
      },
      "outputs": [],
      "source": [
        "# Definições\n",
        "image_size = (224, 224)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bJ8_DuQaCVeP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ8_DuQaCVeP",
        "outputId": "abea50c0-9d7d-480e-f156-cf44efc3a4c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8627 images belonging to 202 classes.\n",
            "Found 2157 images belonging to 202 classes.\n",
            "X_train batch shape: (32, 224, 224, 3), y_train batch shape: (32, 202)\n",
            "X_val batch shape: (32, 224, 224, 3), y_val batch shape: (32, 202)\n",
            "Num classes: 202\n"
          ]
        }
      ],
      "source": [
        "train_dataset = train_aug.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_dataset = val_aug.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Verificar o shape dos batches\n",
        "for X_batch, y_batch in train_dataset:\n",
        "    print(f\"X_train batch shape: {X_batch.shape}, y_train batch shape: {y_batch.shape}\")\n",
        "    break\n",
        "\n",
        "for X_batch, y_batch in val_dataset:\n",
        "    print(f\"X_val batch shape: {X_batch.shape}, y_val batch shape: {y_batch.shape}\")\n",
        "    break\n",
        "\n",
        "# Número de classes\n",
        "num_classes = train_dataset.num_classes\n",
        "print(f\"Num classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sAtcQbTJByCO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAtcQbTJByCO",
        "outputId": "c3fbceb2-0bc6-4a05-ce34-9fa07408578d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights calculados: {0: np.float64(0.39544371103777043), 1: np.float64(1.9412691269126914), 2: np.float64(0.9932074602809118), 3: np.float64(2.0337105139085336), 4: np.float64(2.0337105139085336), 5: np.float64(0.20434411862238855), 6: np.float64(0.9932074602809118), 7: np.float64(1.9412691269126914), 8: np.float64(1.9412691269126914), 9: np.float64(1.9412691269126914), 10: np.float64(1.9412691269126914), 11: np.float64(1.9412691269126914), 12: np.float64(1.9412691269126914), 13: np.float64(1.9412691269126914), 14: np.float64(1.9412691269126914), 15: np.float64(1.9412691269126914), 16: np.float64(1.9412691269126914), 17: np.float64(0.5024461269656377), 18: np.float64(0.6570449352627571), 19: np.float64(0.9932074602809118), 20: np.float64(1.9412691269126914), 21: np.float64(2.0337105139085336), 22: np.float64(2.0337105139085336), 23: np.float64(0.9932074602809118), 24: np.float64(0.9932074602809118), 25: np.float64(0.33106915342697063), 26: np.float64(0.9932074602809118), 27: np.float64(0.9932074602809118), 28: np.float64(0.9932074602809118), 29: np.float64(2.0337105139085336), 30: np.float64(0.39544371103777043), 31: np.float64(1.9412691269126914), 32: np.float64(2.0337105139085336), 33: np.float64(1.9412691269126914), 34: np.float64(0.9932074602809118), 35: np.float64(2.0337105139085336), 36: np.float64(0.24686659417386825), 37: np.float64(1.9412691269126914), 38: np.float64(0.28283391253032586), 39: np.float64(0.39544371103777043), 40: np.float64(0.9932074602809118), 41: np.float64(1.9412691269126914), 42: np.float64(0.4966037301404559), 43: np.float64(1.9412691269126914), 44: np.float64(1.9412691269126914), 45: np.float64(0.2201439216086557), 46: np.float64(2.0337105139085336), 47: np.float64(0.6570449352627571), 48: np.float64(0.19772185551888521), 49: np.float64(1.9412691269126914), 50: np.float64(1.9412691269126914), 51: np.float64(0.9932074602809118), 52: np.float64(1.0416566046848588), 53: np.float64(0.9932074602809118), 54: np.float64(2.0337105139085336), 55: np.float64(1.0416566046848588), 56: np.float64(1.9412691269126914), 57: np.float64(1.9412691269126914), 58: np.float64(0.6570449352627571), 59: np.float64(0.6570449352627571), 60: np.float64(2.0337105139085336), 61: np.float64(0.9932074602809118), 62: np.float64(1.9412691269126914), 63: np.float64(0.9932074602809118), 64: np.float64(0.9932074602809118), 65: np.float64(1.9412691269126914), 66: np.float64(1.9412691269126914), 67: np.float64(1.9412691269126914), 68: np.float64(0.19772185551888521), 69: np.float64(1.9412691269126914), 70: np.float64(0.39544371103777043), 71: np.float64(1.9412691269126914), 72: np.float64(1.9412691269126914), 73: np.float64(1.9412691269126914), 74: np.float64(0.4966037301404559), 75: np.float64(1.9412691269126914), 76: np.float64(0.33106915342697063), 77: np.float64(0.9932074602809118), 78: np.float64(1.9412691269126914), 79: np.float64(0.9932074602809118), 80: np.float64(1.9412691269126914), 81: np.float64(1.9412691269126914), 82: np.float64(0.9932074602809118), 83: np.float64(0.9932074602809118), 84: np.float64(2.0337105139085336), 85: np.float64(1.9412691269126914), 86: np.float64(0.6570449352627571), 87: np.float64(1.9412691269126914), 88: np.float64(2.0337105139085336), 89: np.float64(1.9412691269126914), 90: np.float64(1.9412691269126914), 91: np.float64(1.9412691269126914), 92: np.float64(1.9412691269126914), 93: np.float64(0.9932074602809118), 94: np.float64(2.0337105139085336), 95: np.float64(1.9412691269126914), 96: np.float64(0.6570449352627571), 97: np.float64(2.0337105139085336), 98: np.float64(0.4966037301404559), 99: np.float64(1.9412691269126914), 100: np.float64(0.9932074602809118), 101: np.float64(0.9932074602809118), 102: np.float64(1.9412691269126914), 103: np.float64(0.39544371103777043), 104: np.float64(1.9412691269126914), 105: np.float64(0.9932074602809118), 106: np.float64(0.9932074602809118), 107: np.float64(1.9412691269126914), 108: np.float64(0.9932074602809118), 109: np.float64(2.0337105139085336), 110: np.float64(2.0337105139085336), 111: np.float64(1.9412691269126914), 112: np.float64(2.0337105139085336), 113: np.float64(1.9412691269126914), 114: np.float64(2.0337105139085336), 115: np.float64(1.9412691269126914), 116: np.float64(0.6570449352627571), 117: np.float64(0.6570449352627571), 118: np.float64(1.9412691269126914), 119: np.float64(1.9412691269126914), 120: np.float64(0.9932074602809118), 121: np.float64(1.9412691269126914), 122: np.float64(1.9412691269126914), 123: np.float64(2.0337105139085336), 124: np.float64(1.9412691269126914), 125: np.float64(1.9412691269126914), 126: np.float64(2.0337105139085336), 127: np.float64(1.9412691269126914), 128: np.float64(1.9412691269126914), 129: np.float64(1.9412691269126914), 130: np.float64(2.0337105139085336), 131: np.float64(1.9412691269126914), 132: np.float64(0.2201439216086557), 133: np.float64(1.9412691269126914), 134: np.float64(1.9412691269126914), 135: np.float64(1.9412691269126914), 136: np.float64(1.9412691269126914), 137: np.float64(1.9412691269126914), 138: np.float64(2.0337105139085336), 139: np.float64(0.6570449352627571), 140: np.float64(0.4966037301404559), 141: np.float64(0.9932074602809118), 142: np.float64(0.9932074602809118), 143: np.float64(0.6570449352627571), 144: np.float64(2.0337105139085336), 145: np.float64(2.0337105139085336), 146: np.float64(0.6570449352627571), 147: np.float64(1.9412691269126914), 148: np.float64(0.9932074602809118), 149: np.float64(1.9412691269126914), 150: np.float64(0.9932074602809118), 151: np.float64(0.2201439216086557), 152: np.float64(2.0337105139085336), 153: np.float64(1.9412691269126914), 154: np.float64(1.9412691269126914), 155: np.float64(0.9932074602809118), 156: np.float64(2.0337105139085336), 157: np.float64(0.6570449352627571), 158: np.float64(2.0337105139085336), 159: np.float64(1.9412691269126914), 160: np.float64(1.9412691269126914), 161: np.float64(0.9932074602809118), 162: np.float64(0.4966037301404559), 163: np.float64(0.6570449352627571), 164: np.float64(2.0337105139085336), 165: np.float64(1.9412691269126914), 166: np.float64(1.9412691269126914), 167: np.float64(1.9412691269126914), 168: np.float64(0.9932074602809118), 169: np.float64(0.9932074602809118), 170: np.float64(0.6570449352627571), 171: np.float64(2.0337105139085336), 172: np.float64(0.6570449352627571), 173: np.float64(1.9412691269126914), 174: np.float64(1.9412691269126914), 175: np.float64(0.9932074602809118), 176: np.float64(1.9412691269126914), 177: np.float64(0.9932074602809118), 178: np.float64(1.9412691269126914), 179: np.float64(0.9932074602809118), 180: np.float64(2.0337105139085336), 181: np.float64(2.0337105139085336), 182: np.float64(2.0337105139085336), 183: np.float64(0.28283391253032586), 184: np.float64(0.4966037301404559), 185: np.float64(2.0337105139085336), 186: np.float64(1.9412691269126914), 187: np.float64(0.6570449352627571), 188: np.float64(0.9932074602809118), 189: np.float64(1.9412691269126914), 190: np.float64(2.0337105139085336), 191: np.float64(2.0337105139085336), 192: np.float64(2.0337105139085336), 193: np.float64(0.9932074602809118), 194: np.float64(0.9932074602809118), 195: np.float64(2.0337105139085336), 196: np.float64(0.9932074602809118), 197: np.float64(0.9932074602809118), 198: np.float64(1.9412691269126914), 199: np.float64(1.9412691269126914), 200: np.float64(0.9932074602809118), 201: np.float64(1.9412691269126914)}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "labels = train_dataset.classes\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(labels),\n",
        "    y=labels\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(\"Class weights calculados:\", class_weight_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z8vFZLUwCa_m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8vFZLUwCa_m",
        "outputId": "bbea6d59-2064-4d02-cac2-79a4cffa677b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (115600000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 4.98355, saving model to melhor_modelo.keras\n",
            "270/270 - 408s - 2s/step - AUC: 0.5863 - accuracy: 0.0158 - loss: 5.2980 - val_AUC: 0.7306 - val_accuracy: 0.0890 - val_loss: 4.9835 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 2: val_loss improved from 4.98355 to 4.61814, saving model to melhor_modelo.keras\n",
            "270/270 - 340s - 1s/step - AUC: 0.7228 - accuracy: 0.0626 - loss: 4.9040 - val_AUC: 0.8515 - val_accuracy: 0.2378 - val_loss: 4.6181 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 3: val_loss improved from 4.61814 to 4.11829, saving model to melhor_modelo.keras\n",
            "270/270 - 329s - 1s/step - AUC: 0.8179 - accuracy: 0.1476 - loss: 4.4737 - val_AUC: 0.9143 - val_accuracy: 0.3338 - val_loss: 4.1183 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 4: val_loss improved from 4.11829 to 3.59071, saving model to melhor_modelo.keras\n",
            "270/270 - 382s - 1s/step - AUC: 0.8770 - accuracy: 0.2215 - loss: 3.9582 - val_AUC: 0.9365 - val_accuracy: 0.3802 - val_loss: 3.5907 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 5: val_loss improved from 3.59071 to 3.17503, saving model to melhor_modelo.keras\n",
            "270/270 - 330s - 1s/step - AUC: 0.9039 - accuracy: 0.2811 - loss: 3.4969 - val_AUC: 0.9464 - val_accuracy: 0.4200 - val_loss: 3.1750 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n"
          ]
        }
      ],
      "source": [
        "# === MODELO BASE ===\n",
        "base_model = EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "# === CALLBACKS ===\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "checkpoint = ModelCheckpoint(\"melhor_modelo.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
        "\n",
        "# === TREINO COM BASE CONGELADA ===\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=30,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=2,\n",
        "    class_weight=class_weight_dict\n",
        ")\n",
        "\n",
        "# === FINE-TUNING ===\n",
        "base_model.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", \"AUC\"]\n",
        ")\n",
        "\n",
        "fine_tune_history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=2,\n",
        "    class_weight=class_weight_dict\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0eab3b56",
        "MHZzl13FPK1X",
        "Gb3tyfj_cs7k"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
