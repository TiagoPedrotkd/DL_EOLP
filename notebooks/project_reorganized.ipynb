{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c40d1e",
   "metadata": {},
   "source": [
    "#  Projeto Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab3b56",
   "metadata": {},
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60948300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import TypeVar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,Flatten,Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fee9c",
   "metadata": {},
   "source": [
    "## Carregamento do CSV de Metadados e verificação de imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63baac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"metadata.csv\" \n",
    "meta_data = pd.read_csv(metadata_path)\n",
    "\n",
    "base_dir = \"/Users/joaosantos/Documents/Mestrado Joao/2 semestre/Deep Learning/rare_species 1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e7ea26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image_exists(image_file):\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if image_file in files:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f0f55",
   "metadata": {},
   "source": [
    "Verificar se as imagens mencionada no Csv estão presentes nas pastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8719bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_images = []\n",
    "for index, row in meta_data.iterrows():\n",
    "    image_path = row['file_path']  \n",
    "    image_name = os.path.basename(image_path)  \n",
    "    if not check_image_exists(image_name):\n",
    "        missing_images.append(image_path)\n",
    "\n",
    "print(f\"Número de imagens em falta: {len(missing_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388756dc",
   "metadata": {},
   "source": [
    "Aqui é que conta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99996dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_local_images(base_dir, image_size=(32, 32)):\n",
    "    \"\"\"\n",
    "    Carrega imagens a partir de um diretório e as prepara para o treinamento.\n",
    "\n",
    "    Parâmetros:\n",
    "    - base_dir (str): Caminho do diretório onde as imagens estão armazenadas.\n",
    "    - image_size (tuple): Tamanho para redimensionamento das imagens.\n",
    "\n",
    "    Retorna:\n",
    "    - X_data (np.array): Imagens normalizadas.\n",
    "    - y_data (np.array): Rótulos One-Hot encoded.\n",
    "    \"\"\"\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    # Obter as classes (nomes das pastas) e garantir que são pastas de imagens\n",
    "    labels = os.listdir(base_dir)\n",
    "    labels = [label for label in labels if os.path.isdir(os.path.join(base_dir, label)) and not label.startswith('.')]  # Ignora arquivos como '.DS_Store'\n",
    "    \n",
    "    # Criar um mapeamento de classe (nome da pasta) para um índice inteiro\n",
    "    label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    for label in labels:\n",
    "        class_dir = os.path.join(base_dir, label)\n",
    "        if os.path.isdir(class_dir):  # Verificar se é um diretório\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                # Carregar a imagem\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    print(f\"Erro ao carregar a imagem: {img_path}\")\n",
    "                    continue  # Ignorar a imagem se não for carregada corretamente\n",
    "                # Redimensionar a imagem para 32x32\n",
    "                img = cv2.resize(img, image_size)\n",
    "                X_data.append(img)\n",
    "                y_data.append(label_map[label])  # O rótulo é o índice da pasta (classe)\n",
    "    \n",
    "    # Converter listas para numpy arrays e normalizar\n",
    "    X_data = np.array(X_data, dtype=\"float32\") / 255.0\n",
    "    y_data = np.array(y_data)\n",
    "    \n",
    "    # One-hot encoding para os rótulos\n",
    "    num_classes = len(labels)  # Aqui, pegamos dinamicamente o número de classes\n",
    "    y_data = to_categorical(y_data, num_classes=num_classes)  # One-hot encoding com o número correto de classes\n",
    "\n",
    "    return X_data, y_data\n",
    "\n",
    "# Caminho para o diretório com as imagens\n",
    "base_dir = \"/Users/joaosantos/Documents/Mestrado Joao/2 semestre/Deep Learning/rare_species 1\"\n",
    "\n",
    "# Carregar as imagens locais\n",
    "X_train, y_train = load_local_images(base_dir)\n",
    "\n",
    "# Convertendo os rótulos para tf.float32 (caso necessário)\n",
    "y_train = tf.cast(y_train, tf.float32)\n",
    "\n",
    "# Verificar o número de classes\n",
    "num_classes = y_train.shape[1]  # Será 203 ou o número de pastas que você tem\n",
    "\n",
    "# Visualizar as dimensões dos dados\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, Número de classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee10db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carregar as imagens\n",
    "X_data, y_data = load_local_images(base_dir)\n",
    "\n",
    "# Dividir os dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verifique as formas dos conjuntos de dados\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast labels\n",
    "y_train = tf.cast(y_train, tf.float32)\n",
    "y_test = tf.cast(y_test, tf.float32)\n",
    "\n",
    "# Dataset TensorFlow\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "validation_steps = len(X_test) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af78c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste no modelo para refletir o número de classes correto\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Modelo convolucional otimizado\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Agora ajustado dinamicamente para o número correto de classes\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", \"AUC\"]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\"melhor_modelo.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
    "\n",
    "# Treinamento\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00470575",
   "metadata": {},
   "source": [
    "Deu 0.18 de accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11819c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Carregar o modelo salvo\n",
    "modelo_final = load_model(\"melhor_modelo.keras\")\n",
    "\n",
    "# Previsões\n",
    "y_pred_probs = modelo_final.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Matriz de confusão\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap=\"Blues\")\n",
    "plt.title(\"Matriz de Confusão\")\n",
    "plt.xlabel(\"Previsto\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
