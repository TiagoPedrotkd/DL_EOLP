{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "057bbc14",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 6px solid #FF8C42; padding:20px; border-radius:10px; font-family:Arial, sans-serif; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  âš™ï¸ 04 â€“ Hyperparameter Tuning\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600257f8",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 6px solid #27ae60; margin-left:40px; padding:10px; border-radius:10px; font-family:Arial, sans-serif; font-size:24px;\">\n",
    "  <h2 style=\"margin-top: 0; font-size:24px;\">ğŸ“¦ Import Libraries and Define Paths</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left:60px; padding:10px;\"> \n",
    "  <p style=\"font-size:18px;\">This is the initial block of the rare species image classification project.</p>\n",
    "\n",
    "  <p>In this section, we perform the following tasks:</p>\n",
    "\n",
    "  <ul style=\"line-height: 1.6;\">\n",
    "    <li>ğŸ“ <strong>Import libraries</strong> for data manipulation (<code>pandas</code>), file paths (<code>pathlib</code>), and image processing (<code>PIL</code>).</li>\n",
    "    <li>ğŸ–¼ï¸ <strong>Apply visual styling</strong> using <code>matplotlib</code> and <code>seaborn</code> to ensure clean and consistent plots.</li>\n",
    "    <li>ğŸ“‚ <strong>Define the main project directories</strong>, including image folders and the metadata CSV file.</li>\n",
    "    <li>âœ… <strong>Automatic path validation</strong> to ensure all required files and directories exist.</li>\n",
    "  </ul>\n",
    "\n",
    "  <p>This setup provides a reliable foundation for safely loading and exploring the dataset.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82830fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.applications import MobileNetV2, VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193efc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path().resolve().parent\n",
    "\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "REPORTS_DIR = PROJECT_ROOT / 'reports'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'output'\n",
    "LOGS_DIR = OUTPUTS_DIR / 'logs'\n",
    "PREDICTIONS_DIR = OUTPUTS_DIR / 'predictions'\n",
    "TRAIN_DIR = PROCESSED_DIR / 'train'\n",
    "VAL_DIR = PROCESSED_DIR / 'val'\n",
    "TEST_DIR = PROCESSED_DIR / 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034564d4",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 6px solid #27ae60; margin-left:40px; padding:10px; border-radius:10px; font-family:Arial, sans-serif; font-size:24px;\">\n",
    "  <h2 style=\"margin-top: 0; font-size:24px;\">ğŸ“¦ Define Parameters</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left:60px; padding:10px;\"> \n",
    "  <p>In this section, we define the core parameters that will guide the training process of the model. These include the input image size, batch size, number of training epochs, and the directory structure of the dataset.</p>\n",
    "  \n",
    "  <p>Setting these values early ensures consistency across all steps and allows for easier adjustments when experimenting with different model architectures or datasets.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9dafeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eae14d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 6px solid #27ae60; margin-left:40px; padding:10px; border-radius:10px; font-family:Arial, sans-serif; font-size:24px;\">\n",
    "  <h2 style=\"margin-top: 0; font-size:24px;\">ğŸ“‚ Load and Prepare the Dataset</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left:60px; padding:10px; font-family:Arial, sans-serif; font-size:16px;\"> \n",
    "  <p>This section is responsible for loading the processed dataset and preparing it for training and evaluation.</p>\n",
    "\n",
    "  <p>Using <code>ImageDataGenerator</code>, the images are normalized (pixel values scaled between 0 and 1), and loaded in batches directly from the respective folders for:</p>\n",
    "\n",
    "  <ul style=\"line-height: 1.6;\">\n",
    "    <li><strong>ğŸŸ¢ Training set</strong> â€” used to update model weights during learning</li>\n",
    "    <li><strong>ğŸŸ  Validation set</strong> â€” used to monitor generalization and prevent overfitting</li>\n",
    "    <li><strong>ğŸ”µ Test set</strong> â€” used for final evaluation after training</li>\n",
    "  </ul>\n",
    "\n",
    "  <p>The dataset is expected to be organized in subfolders where each folder represents one class label.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae733941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8462 images belonging to 202 classes.\n",
      "Found 2157 images belonging to 202 classes.\n",
      "Found 1199 images belonging to 202 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(TRAIN_DIR, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "val_generator = datagen.flow_from_directory(VAL_DIR, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "test_generator = datagen.flow_from_directory(TEST_DIR, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n",
    "\n",
    "NUM_CLASSES = train_generator.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f28d9fa",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 6px solid #27ae60; margin-left:40px; padding:10px; border-radius:10px; font-family:Arial, sans-serif; font-size:24px;\">\n",
    "  <h2 style=\"margin-top: 0; font-size:24px;\">âš™ï¸ Set Up Hyperparameter Ranges for MobileNetV2 and VGG16</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631fafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mobilenetv2(hp):\n",
    "    base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='avg')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(hp.Int('units1', min_value=128, max_value=1024, step=64), activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout1', 0.3, 0.6, step=0.1)))\n",
    "\n",
    "    model.add(Dense(hp.Int('units2', min_value=64, max_value=512, step=64), activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout2', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 5e-4, 1e-4])\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "413060f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from D:\\Repositories\\DL_EOLP\\output\\logs\\tuner_logs\\mobilenetv2_tuning\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_mobilenetv2,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=EPOCHS,\n",
    "    factor=3,\n",
    "    directory= LOGS_DIR / 'tuner_logs',\n",
    "    project_name='mobilenetv2_tuning'\n",
    ")\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=3)\n",
    "tuner.search(train_generator, validation_data=val_generator, epochs=EPOCHS, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bb394b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Tiago Pedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago Pedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparÃ¢metros encontrados:\n",
      "{'units1': 1024, 'dropout1': 0.4, 'units2': 256, 'dropout2': 0.2, 'learning_rate': 0.0005, 'tuner/epochs': 17, 'tuner/initial_epoch': 6, 'tuner/bracket': 3, 'tuner/round': 2, 'tuner/trial_id': '0034'}\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparams = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"Melhores hiperparÃ¢metros encontrados:\")\n",
    "print(best_hyperparams.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87352770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mobilenetv2_tuned_pipeline(train_gen, val_gen, test_gen, model_name=\"mobilenetv2_tuned\", image_size=(224, 224), epochs=20):\n",
    "    models_dir = MODELS_DIR\n",
    "    logs_dir = LOGS_DIR\n",
    "    predictions_dir = PREDICTIONS_DIR\n",
    "    reports_dir = REPORTS_DIR\n",
    "    figures_dir = reports_dir / \"figures\"\n",
    "    \n",
    "    for d in [models_dir, logs_dir, predictions_dir, figures_dir, reports_dir]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_generator = train_gen\n",
    "    val_generator = val_gen\n",
    "    test_generator = test_gen\n",
    "    num_classes = train_generator.num_classes\n",
    "\n",
    "    base_model = MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=(image_size[0], image_size[1], 3), pooling='avg')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        BatchNormalization(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
    "    csv_logger = CSVLogger(logs_dir / f\"{model_name}_training_log.csv\", append=False)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=[csv_logger, early_stop, reduce_lr]\n",
    "    )\n",
    "\n",
    "    model_path = models_dir / f\"{model_name}.h5\"\n",
    "    model_weights_path = models_dir / f\"{model_name}.weights.h5\"\n",
    "    model.save(model_path)\n",
    "    model.save_weights(model_weights_path)\n",
    "\n",
    "    val_loss, val_acc = model.evaluate(val_generator)\n",
    "\n",
    "    acc_fig_path = figures_dir / f\"{model_name}_accuracy_plot.png\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(acc_fig_path)\n",
    "    plt.close()\n",
    "\n",
    "    predictions = model.predict(test_generator)\n",
    "    predicted_classes = predictions.argmax(axis=1)\n",
    "    true_classes = test_generator.classes\n",
    "    class_indices = test_generator.class_indices\n",
    "    inv_class_indices = {v: k for k, v in class_indices.items()}\n",
    "    predicted_labels = [inv_class_indices[i] for i in predicted_classes]\n",
    "    true_labels = [inv_class_indices[i] for i in true_classes]\n",
    "\n",
    "    report = classification_report(true_classes, predicted_classes, target_names=list(class_indices.keys()), output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_path = reports_dir / f\"{model_name}_classification_report.csv\"\n",
    "    report_df.to_csv(report_path)\n",
    "\n",
    "    heatmap_path = figures_dir / f\"{model_name}_classification_report_heatmap_top20.png\"\n",
    "    filtered_df = report_df.drop([\"accuracy\", \"macro avg\", \"weighted avg\"], errors=\"ignore\")\n",
    "    top_20 = filtered_df.sort_values(\"support\", ascending=False).head(20)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(top_20[[\"precision\", \"recall\", \"f1-score\"]], annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=0.5, annot_kws={\"size\": 9})\n",
    "    plt.title(\"Top 20 Classes â€“ Classification Report\", fontsize=14)\n",
    "    plt.xlabel(\"Metrics\", fontsize=12)\n",
    "    plt.ylabel(\"Class\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(heatmap_path)\n",
    "    plt.close()\n",
    "\n",
    "    top_labels = list(top_20.index)\n",
    "    label_to_index = {name: i for i, name in enumerate(class_indices.keys())}\n",
    "    top_indices = [label_to_index[l] for l in top_labels]\n",
    "    filtered_true = [i for i in true_classes if i in top_indices]\n",
    "    filtered_pred = [p for i, p in enumerate(predicted_classes) if true_classes[i] in top_indices]\n",
    "    \n",
    "    cm = confusion_matrix(filtered_true, filtered_pred, labels=top_indices)\n",
    "    cm_labels = [list(class_indices.keys())[i] for i in top_indices]\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cm_labels)\n",
    "    disp.plot(ax=ax, xticks_rotation=45, cmap='Blues', colorbar=True)\n",
    "    plt.title(\"Confusion Matrix â€“ Top 20 Classes\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    cm_path = figures_dir / f\"{model_name}_confusion_matrix_top20.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(class_indices.keys()))\n",
    "    disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')\n",
    "    full_cm_path = figures_dir / f\"{model_name}_confusion_matrix.png\"\n",
    "    plt.savefig(full_cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    filenames = test_generator.filenames\n",
    "    results_df = pd.DataFrame({\n",
    "        \"filename\": filenames,\n",
    "        \"true_label\": true_labels,\n",
    "        \"predicted_label\": predicted_labels\n",
    "    })\n",
    "    pred_path = predictions_dir / f\"{model_name}_predictions.csv\"\n",
    "    results_df.to_csv(pred_path, index=False)\n",
    "\n",
    "    return {\n",
    "        \"model_path\": model_path,\n",
    "        \"log_path\": logs_dir / f\"{model_name}_training_log.csv\",\n",
    "        \"report_path\": report_path,\n",
    "        \"heatmap_path\": heatmap_path,\n",
    "        \"confusion_matrix\": full_cm_path,\n",
    "        \"predictions_path\": pred_path,\n",
    "        \"accuracy_plot\": acc_fig_path,\n",
    "        \"val_accuracy\": val_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c46171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 147ms/step - accuracy: 0.1324 - loss: 4.6196 - val_accuracy: 0.3922 - val_loss: 2.6700 - learning_rate: 5.0000e-04\n",
      "Epoch 2/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 142ms/step - accuracy: 0.4230 - loss: 2.3898 - val_accuracy: 0.4798 - val_loss: 2.1675 - learning_rate: 5.0000e-04\n",
      "Epoch 3/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 143ms/step - accuracy: 0.5785 - loss: 1.6510 - val_accuracy: 0.5192 - val_loss: 1.9781 - learning_rate: 5.0000e-04\n",
      "Epoch 4/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 164ms/step - accuracy: 0.6657 - loss: 1.2260 - val_accuracy: 0.5364 - val_loss: 1.9839 - learning_rate: 5.0000e-04\n",
      "Epoch 5/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 163ms/step - accuracy: 0.7395 - loss: 0.9285 - val_accuracy: 0.5633 - val_loss: 1.9464 - learning_rate: 5.0000e-04\n",
      "Epoch 6/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 168ms/step - accuracy: 0.7809 - loss: 0.7435 - val_accuracy: 0.5517 - val_loss: 2.0373 - learning_rate: 5.0000e-04\n",
      "Epoch 7/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 161ms/step - accuracy: 0.8140 - loss: 0.6039 - val_accuracy: 0.5586 - val_loss: 2.0178 - learning_rate: 5.0000e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8547 - loss: 0.4770\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 165ms/step - accuracy: 0.8547 - loss: 0.4770 - val_accuracy: 0.5610 - val_loss: 2.1474 - learning_rate: 5.0000e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 164ms/step - accuracy: 0.8892 - loss: 0.3574 - val_accuracy: 0.5684 - val_loss: 2.0577 - learning_rate: 2.5000e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 151ms/step - accuracy: 0.9190 - loss: 0.2731 - val_accuracy: 0.5786 - val_loss: 2.0536 - learning_rate: 2.5000e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9340 - loss: 0.2355\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 155ms/step - accuracy: 0.9340 - loss: 0.2355 - val_accuracy: 0.5860 - val_loss: 2.0723 - learning_rate: 2.5000e-04\n",
      "Epoch 12/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 167ms/step - accuracy: 0.9423 - loss: 0.1996 - val_accuracy: 0.5823 - val_loss: 2.0750 - learning_rate: 1.2500e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 176ms/step - accuracy: 0.9529 - loss: 0.1671 - val_accuracy: 0.5851 - val_loss: 2.0945 - learning_rate: 1.2500e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9608 - loss: 0.1435\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 167ms/step - accuracy: 0.9607 - loss: 0.1436 - val_accuracy: 0.5855 - val_loss: 2.1248 - learning_rate: 1.2500e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 163ms/step - accuracy: 0.9615 - loss: 0.1333 - val_accuracy: 0.5925 - val_loss: 2.0994 - learning_rate: 6.2500e-05\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m68/68\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 132ms/step - accuracy: 0.5572 - loss: 2.0058\n",
      "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 690ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago Pedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Tiago Pedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Tiago Pedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ MobileNetV2 Tuned â€“ Results Summary:\n",
      "\n",
      "ğŸ“ Model saved at:              D:\\Repositories\\DL_EOLP\\models\\mobilenetv2_tuned.h5\n",
      "ğŸ“„ Training log:                D:\\Repositories\\DL_EOLP\\output\\logs\\mobilenetv2_tuned_training_log.csv\n",
      "ğŸ“Š Classification report (CSV): D:\\Repositories\\DL_EOLP\\reports\\mobilenetv2_tuned_classification_report.csv\n",
      "ğŸ§¯ Report heatmap (Top 20):     D:\\Repositories\\DL_EOLP\\reports\\figures\\mobilenetv2_tuned_classification_report_heatmap_top20.png\n",
      "ğŸ“‰ Confusion matrix (full):     D:\\Repositories\\DL_EOLP\\reports\\figures\\mobilenetv2_tuned_confusion_matrix.png\n",
      "ğŸ“ˆ Accuracy plot:               D:\\Repositories\\DL_EOLP\\reports\\figures\\mobilenetv2_tuned_accuracy_plot.png\n",
      "ğŸ“‘ Predictions CSV:             D:\\Repositories\\DL_EOLP\\output\\predictions\\mobilenetv2_tuned_predictions.csv\n",
      "âœ… Final validation accuracy:   56.33%\n"
     ]
    }
   ],
   "source": [
    "results_mobilenetv2_tuned = run_mobilenetv2_tuned_pipeline(\n",
    "    train_gen=train_generator,\n",
    "    val_gen=val_generator,\n",
    "    test_gen=test_generator,\n",
    "    model_name=\"mobilenetv2_tuned\",\n",
    "    image_size=IMAGE_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"ğŸ“¦ MobileNetV2 Tuned â€“ Results Summary:\\n\")\n",
    "print(f\"ğŸ“ Model saved at:              {results_mobilenetv2_tuned['model_path']}\")\n",
    "print(f\"ğŸ“„ Training log:                {results_mobilenetv2_tuned['log_path']}\")\n",
    "print(f\"ğŸ“Š Classification report (CSV): {results_mobilenetv2_tuned['report_path']}\")\n",
    "print(f\"ğŸ§¯ Report heatmap (Top 20):     {results_mobilenetv2_tuned['heatmap_path']}\")\n",
    "print(f\"ğŸ“‰ Confusion matrix (full):     {results_mobilenetv2_tuned['confusion_matrix']}\")\n",
    "print(f\"ğŸ“ˆ Accuracy plot:               {results_mobilenetv2_tuned['accuracy_plot']}\")\n",
    "print(f\"ğŸ“‘ Predictions CSV:             {results_mobilenetv2_tuned['predictions_path']}\")\n",
    "print(f\"âœ… Final validation accuracy:   {results_mobilenetv2_tuned['val_accuracy']:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
